{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712c0358",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 1 -- Machine Learning and NLP Basis\n",
    "\n",
    "#### Name: Agamdeep S. Chopra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1881fbb",
   "metadata": {},
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the preprocessing.\n",
    "3. Implement tokenization.\n",
    "4. Implement feature extraction.\n",
    "5. Implement Logistic Regression.\n",
    "6. Implement Stochastic Gradient Descent and Mini-batch Gradient Descent.\n",
    "7. Evaluate all the experiments and compare all the results.\n",
    "\n",
    "*** Please read the code very carefully and install these packages (NumPy, Pandas, sklearn, tqdm, and matplotlib) before you start ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140e7f",
   "metadata": {},
   "source": [
    "## 1. Data Processing (30 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data by using Pandas\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef5bf9",
   "metadata": {},
   "source": [
    "### 1.1 Load Data\n",
    "\n",
    "Run the following cells (Please make sure the paths of data files are correct.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622bf755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv', header=None)\n",
    "train_df.columns = ['label', 'title', 'text']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b302a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3                  Fears for T N pension after talks   \n",
       "1      4  The Race is On: Second Private Team Sets Launc...   \n",
       "2      4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3      4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4      4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "\n",
       "                                                text  \n",
       "0  Unions representing workers at Turner   Newall...  \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
       "2  AP - A company founded by a chemistry research...  \n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...  \n",
       "4  AP - Southern California's smog-fighting agenc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/test.csv', header=None)\n",
    "test_df.columns = ['label', 'title', 'text']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c78934",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess (Fill the code: 10 points)\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5337a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocesser(object):\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "    \n",
    "    def apply(self, text):\n",
    "        if self.url:\n",
    "            text = self._remove_url(text)\n",
    "            \n",
    "        if self.punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "            \n",
    "        if self.number:\n",
    "            text = self._remove_number(text)\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "        \n",
    "    def _remove_punctuation(self, text):\n",
    "        ''' Please fill this function to remove all the punctuations in the text\n",
    "        '''\n",
    "        return text.translate(str.maketrans('','', string.punctuation))\n",
    "    \n",
    "    def _remove_url(self, text):\n",
    "        ''' Please fill this function to remove all the urls in the text\n",
    "        '''\n",
    "        return re.sub(r'http\\S+','',text)\n",
    "    \n",
    "    def _remove_number(self, text):\n",
    "        ''' Please fill this function to remove all the numbers in the text\n",
    "        '''\n",
    "        return text.translate(str.maketrans('','', string.digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b56b0e",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3ded4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"Interest rates are trimmed to by the South African central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "text = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processer = Preprocesser()\n",
    "clean_text = processer.apply(text)\n",
    "\n",
    "print(f'\"{text}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_text}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855036c5",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86930002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lemma=False):\n",
    "    ''' Please fill this function to tokenize text.\n",
    "    '''\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129417b",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d0f7419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets. ==> ['Interest', 'rates', 'are', 'trimmed', 'to', 'by', 'the', 'South', 'African', 'central', 'bank', 'but', 'the', 'lack', 'of', 'warning', 'hits', 'the', 'rand', 'and', 'surprises', 'markets']\n"
     ]
    }
   ],
   "source": [
    "text = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processer = Preprocesser()\n",
    "clean_text = processer.apply(text)\n",
    "tokens = tokenize(clean_text)\n",
    "\n",
    "print(f'{text} ==> {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6f42e",
   "metadata": {},
   "source": [
    "### 1.4 Data split (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530acb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: 112400\n",
      "The size of validation set: 7600\n",
      "The size of test set: 7600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train = train_df['text'].values.astype(str)\n",
    "label_train = train_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "text_test = test_df['text'].values.astype(str)\n",
    "label_test = test_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "\n",
    "### Start your code, split the text_train and label_train into training and validation\n",
    "### Make sure the names of varables are \"text_train\", \"label_train\", \"text_valid\", and \"label_valid\"\n",
    "\n",
    "text_train, text_valid, label_train, label_valid = train_test_split(text_train,label_train,test_size = 0.06333)\n",
    "\n",
    "### End\n",
    "\n",
    "print('The size of training set:', text_train.shape[0])\n",
    "print('The size of validation set:', text_valid.shape[0])\n",
    "print('The size of test set:', text_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0873ff9",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0967d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class TfIdfExtractor(object):\n",
    "    \n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab = defaultdict(lambda: 0)\n",
    "        self.word2idx = {}\n",
    "        self.df = defaultdict(lambda: 0)\n",
    "        self.num_doc = 0\n",
    "        \n",
    "        self.processer = Preprocesser()\n",
    "        \n",
    "        \n",
    "    def fit(self, texts):\n",
    "        ''' In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary (self.vocab).\n",
    "                2. Construct the document frequency dictionary (self.df).\n",
    "                3. Sort the vocabulary based on the frequence (self.vocab).\n",
    "            Input:\n",
    "                texts: a list of text (training set)\n",
    "            Output:\n",
    "                None\n",
    "        '''\n",
    "\n",
    "        self.num_doc = len(texts)\n",
    "        \n",
    "        for text in tqdm(texts, desc='fitting text'):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)\n",
    "            \n",
    "            ### Start your code (step 1 & 2)\n",
    "            for i in tokens:\n",
    "                self.vocab[i]\n",
    "                self.df[i] += 1\n",
    "            ### End\n",
    "        \n",
    "        ### Start your code (Step 3)\n",
    "        self.vocab = dict(sorted(self.vocab.items(), key=lambda x: self.df[x[0]], reverse=True))\n",
    "        #print(self.vocab)\n",
    "        ### End\n",
    "        \n",
    "        if self.vocab_size is not None:\n",
    "            self.vocab = {key: self.vocab[key] for key in list(self.vocab.keys())[:self.vocab_size]}\n",
    "        \n",
    "        self.word2idx = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
    "        \n",
    "        #print(self.vocab)\n",
    "        #print(self.word2idx)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        ''' In this function, you need to encode the input text into TF-IDF vector.\n",
    "            Input:\n",
    "                texts: a list of text.\n",
    "            Ouput:\n",
    "                a N-d matrix (Tf-Idf) \n",
    "        '''\n",
    "        tfidf = np.zeros((len(texts), len(self.vocab)))\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), desc='transforming', total=len(texts)):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)      \n",
    "            tfidf[i,:] = [tokens.count(list(self.vocab.keys())[j]) for j in range(len(self.vocab))]\n",
    "\n",
    "        tfidf = np.log(1+tfidf)*np.log(1+(len(texts))/np.array([self.df[list(self.vocab.keys())[i]] for i in range(len(self.vocab))]))\n",
    "\n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430c8b3",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8684f9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020912ea5ed141acb7892dda5eabefef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54599180917d45aeb39c3400c0dee75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05136773, 0.13414352, 0.08675648, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.10273546, 0.08463514, 0.08675648, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08141593, 0.13414352, 0.08675648, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.08141593, 0.08463514, 0.17351296, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05136773, 0.        , 0.13750577, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08141593, 0.08463514, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = TfIdfExtractor(vocab_size=400)\n",
    "extractor.fit(text_train[:100])\n",
    "X = extractor.transform(text_train[:10])\n",
    "\n",
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d6bdd",
   "metadata": {},
   "source": [
    "#### 1.5.4 Run the following code to obtain the TD-IDF and One-hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c111922d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0851d3c12f346d6af33f41b58aa74cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/112400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51be145539854497a6340e43320b2ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/112400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe54d4d5f99d4b8aaf3496f9e8573a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781e190ea52742c4ac490332518cd6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: (112400, 1000)\n",
      "The size of validation set: (7600, 1000)\n",
      "The size of test set: (7600, 1000)\n"
     ]
    }
   ],
   "source": [
    "# You can change this number to see the difference of the performances. (larger vocab size needs more memory)\n",
    "vocab_size = 1000\n",
    "num_class = 4\n",
    "\n",
    "extractor = TfIdfExtractor(vocab_size=vocab_size)\n",
    "extractor.fit(text_train)\n",
    "\n",
    "x_train = extractor.transform(text_train)\n",
    "x_valid = extractor.transform(text_valid)\n",
    "x_test = extractor.transform(text_test)\n",
    "\n",
    "\n",
    "# convert label to one-hot vector\n",
    "y_train = np.zeros((label_train.shape[0], num_class))\n",
    "y_train[np.arange(label_train.shape[0]), label_train] = 1\n",
    "\n",
    "y_valid = np.zeros((label_valid.shape[0], num_class))\n",
    "y_valid[np.arange(label_valid.shape[0]), label_valid] = 1\n",
    "\n",
    "y_test = np.zeros((label_test.shape[0], num_class))\n",
    "y_test[np.arange(label_test.shape[0]), label_test] = 1\n",
    "\n",
    "\n",
    "print('The size of training set:', x_train.shape)\n",
    "print('The size of validation set:', x_valid.shape)\n",
    "print('The size of test set:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc1b08",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression (60 points)\n",
    "In this section, you are required to implement a Logistic Regression(LR) model with $L_2$ regularization from scratch. \n",
    "\n",
    "\n",
    "The objective function of LR:\n",
    "\n",
    "<center> $J = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$ </center>\n",
    "\n",
    "**Deliverable 1**: Given the objective function, please show the steps to derive the graident of J with respecty of $w_k$. You can either list the steps in the notebook or submit a pdf with all the steps in the submission. **(10 points)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02190f47",
   "metadata": {},
   "source": [
    "Answer: Attached PDF document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d52c19",
   "metadata": {},
   "source": [
    "### 2.1 LR and softmax function (Fill the code, 20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "457d0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: I noticed some bugs in the provided code/instructions that I think I was able to fix. I also simplified the code rewriting the w,b,dw, and db as instance attributes.\n",
    "\n",
    "def softmax(x):\n",
    "    ''' Compute the softmax function for each row of the input x.\n",
    "        \n",
    "        Inputs:\n",
    "            x: A D dimensional vector or N x D dimensional numpy matrix.\n",
    "        Outputs:\n",
    "            x: You are allowed to modify x in-place\n",
    "    '''\n",
    "    ### Start your code\n",
    "    # Using the x - max(x) to prevent overflow\n",
    "    try:\n",
    "        x = np.exp(x - np.max(x))\n",
    "        x = (x.T/np.sum(x, axis=1)).T\n",
    "    except:\n",
    "        x = np.exp(x - np.max(x))\n",
    "        x = x/np.sum(x)\n",
    "    ### End\n",
    "    return x\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_class, lam):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_class = num_class\n",
    "        self.lam = lam\n",
    "        \n",
    "        ### Start your code (initialize weight(w) and bias(b))\n",
    "        ### hint: you could use np.random.rand() to randomly initialize the parameters\n",
    "\n",
    "        self.w = np.random.rand(self.vocab_size, num_class)\n",
    "        self.w_grad = np.zeros(self.w.size)\n",
    "        self.b = np.zeros((1, num_class))\n",
    "        self.b_grad = np.zeros(self.b.size)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "    def objective(self, x, y):\n",
    "        ''' Implement the objective function\n",
    "            Inputs:\n",
    "                x: N-d matrix\n",
    "                y: N-1 vector\n",
    "            Output:\n",
    "                the objective value of LR (scalar)\n",
    "        '''\n",
    "        loss = 0\n",
    "        \n",
    "        ### Start your code\n",
    "        #The loss function simplified to matrix form - the L2 term. Adding the L2 term made the loss values act strenge. I am not sure why.\n",
    "        loss = ((1/x.shape[0])*(np.log(np.sum(np.exp(-x@self.w-self.b),axis=1)) + np.trace((x@self.w+self.b)@y.T)))[0]\n",
    "        ### End\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        ''' Implement the gradient of J with respect to w (in Deliverable 1)\n",
    "            Inputs:\n",
    "                x: N-d matrix\n",
    "                y: N-1 vector\n",
    "            Output:\n",
    "                the gradient of J w.r.t weight (d-K matrix)\n",
    "                the gradient of J w.r.t bias (K vector)\n",
    "        '''\n",
    "        ### Start your code\n",
    "        # The gradient update algorithm in matrix form as derived in the supplimental derivation.pdf file.\n",
    "        self.w_grad = (1/x.shape[0])*(x.T@(y-softmax(-x@self.w-self.b))) + 2*self.lam*self.w\n",
    "        self.b_grad = (1/x.shape[0])*(np.sum((y-softmax(-x@self.w-self.b)),axis=0))\n",
    "        ### End\n",
    "        \n",
    "    \n",
    "    \n",
    "    def gradient_descent(self,x,y, lr):\n",
    "        ''' Implement the graident descent. \n",
    "            Updating weights and bias based on Equation: w = w - learning_rate * gradient)\n",
    "            \n",
    "            Inputs:\n",
    "                w_grad: a matrix which is the gradient of J w.r.t to weight\n",
    "                b_grad: a vector wich is the graident of J w.r.t to bias\n",
    "            Output:\n",
    "                None\n",
    "        '''\n",
    "        ### Start your code\n",
    "        self.gradient(x,y) # <- calculating the gradients\n",
    "        # updating the paramaters with the new gradient values\n",
    "        self.w = self.w - lr*self.w_grad\n",
    "        self.b = self.b - lr*self.b_grad\n",
    "        ### End\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_hat = softmax(np.dot(x, self.w)+self.b).squeeze()\n",
    "        #y_hat = softmax(np.dot(x, self.w)).squeeze() <- for some reason the provided code never used the bias term so I rewrote the prediction equation to include bias.\n",
    "        return np.argmax(y_hat, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22522a",
   "metadata": {},
   "source": [
    "### 2.2 Stochastic Gradient Descent (SGD) (Fill the code, 15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9f8f4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: I restructured and rewrote parts of this code to better fit my implementation.\n",
    "\n",
    "def sgd(model, X, y, Xv, yv, lr, lam, num_epoch=100):\n",
    "    ''' Implement SGD\n",
    "        Inputs:\n",
    "            X: N-d matrix\n",
    "            y: N vector\n",
    "            lr: learning rate\n",
    "            lam: lambda\n",
    "            num_epoch: the number of epochs\n",
    "        Output:\n",
    "            1. A list of training losses against epoch\n",
    "            2. A list of validation losses against epoch\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        ### Start your code here (Please implement SGD and obtain the training loss)\n",
    "        for i in range(n):\n",
    "            if train_loss == 0.:\n",
    "                train_loss = model.objective(X[i].reshape((1,X.shape[1])),y[i].reshape((1,y.shape[1])))\n",
    "            else:\n",
    "                train_loss = (train_loss + model.objective(X[i].reshape((1,X.shape[1])),y[i].reshape((1,y.shape[1]))))/2\n",
    "            model.gradient_descent(X[i].reshape((1,X.shape[1])),y[i].reshape((1,y.shape[1])),lr)\n",
    "        ### End\n",
    "        \n",
    "        valid_loss = 0.\n",
    "        \n",
    "        ### Start your code (Using validation set to obtain the validation loss)\n",
    "        valid_loss = model.objective(Xv,yv)\n",
    "        ### End\n",
    "        \n",
    "        \n",
    "        print(f'At epoch {e+1}, training loss: {train_loss:.9f}, validation loss: {valid_loss:.9f}.')\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "            \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260f156",
   "metadata": {},
   "source": [
    "Run SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9010e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1, training loss: 0.068298847, validation loss: 4.493543308.\n",
      "At epoch 2, training loss: 0.070566364, validation loss: 3.640689023.\n",
      "At epoch 3, training loss: 0.069894862, validation loss: 2.911714886.\n",
      "At epoch 4, training loss: 0.069256109, validation loss: 2.287291679.\n",
      "At epoch 5, training loss: 0.068896805, validation loss: 1.750459541.\n",
      "At epoch 6, training loss: 0.068709072, validation loss: 1.287967819.\n",
      "At epoch 7, training loss: 0.068609820, validation loss: 0.889065430.\n",
      "At epoch 8, training loss: 0.068555645, validation loss: 0.544784366.\n",
      "At epoch 9, training loss: 0.068525189, validation loss: 0.247529093.\n",
      "At epoch 10, training loss: 0.068507732, validation loss: -0.009186418.\n"
     ]
    }
   ],
   "source": [
    "''' Update the hyper-parameters (num_epoch, lr, and lam) according to your observation to achieve better performance.\n",
    "'''\n",
    "num_epoch = 10\n",
    "lr = 0.65E-2\n",
    "lam = 1E-4\n",
    "\n",
    "sgd_lr = LogisticRegression(vocab_size, num_class, lam)\n",
    "sgd_train_losses, sgd_valid_losses = sgd(sgd_lr, x_train, y_train, x_valid, y_valid, lr, lam, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f023491",
   "metadata": {},
   "source": [
    "### 2.3 Mini-batch Gradient Descent (Fill the code: 15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362fca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: I restructured and rewrote parts of this code to better fit my implementation.\n",
    "\n",
    "def mini_batch_gd(model, X, y, Xv, yv, batch_size, lr, lam, num_epoch=100, show=False):\n",
    "    ''' Implement SGD\n",
    "        Inputs:\n",
    "            X: N-d matrix\n",
    "            y: N vector\n",
    "            lr: learning rate\n",
    "            lam: lambda\n",
    "            num_epoch: the number of epochs\n",
    "        Output:\n",
    "            1. A list of training losses against epoch\n",
    "            2. A list of validation losses against epoch\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        for i in range(0,n-batch_size,batch_size):\n",
    "            if train_loss == 0.:\n",
    "                train_loss = model.objective(X[i:i+batch_size],y[i:i+batch_size])\n",
    "            else:\n",
    "                train_loss = (train_loss + model.objective(X[i:i+batch_size],y[i:i+batch_size]))/2\n",
    "            model.gradient_descent(X[i:i+batch_size],y[i:i+batch_size],lr)\n",
    "            \n",
    "        ### End\n",
    "        \n",
    "        valid_loss = 0.\n",
    "        \n",
    "        ### Start your code (Using validation set to obtain the validation loss)\n",
    "        valid_loss = model.objective(Xv,yv)\n",
    "        \n",
    "        train_losses.append(abs(train_loss))\n",
    "        valid_losses.append(abs(valid_loss))\n",
    "        \n",
    "        if show:\n",
    "            print(f'At epoch {e+1}, training loss: {train_loss:.9f}, validation loss: {valid_loss:.9f}.')\n",
    "            \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afba60a",
   "metadata": {},
   "source": [
    "Run Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d94734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1, training loss: 15.602881505, validation loss: 5.728120364.\n",
      "At epoch 2, training loss: 13.211235108, validation loss: 4.838692226.\n",
      "At epoch 3, training loss: 11.304364567, validation loss: 4.130053964.\n",
      "At epoch 4, training loss: 9.726589522, validation loss: 3.544653875.\n",
      "At epoch 5, training loss: 8.388293367, validation loss: 3.049054828.\n",
      "At epoch 6, training loss: 7.234270894, validation loss: 2.622494347.\n",
      "At epoch 7, training loss: 6.227920078, validation loss: 2.251122132.\n",
      "At epoch 8, training loss: 5.343599577, validation loss: 1.925199516.\n",
      "At epoch 9, training loss: 4.562534469, validation loss: 1.637591241.\n",
      "At epoch 10, training loss: 3.870422994, validation loss: 1.382876990.\n",
      "At epoch 11, training loss: 3.255956477, validation loss: 1.156793996.\n",
      "At epoch 12, training loss: 2.709880596, validation loss: 0.955877399.\n",
      "At epoch 13, training loss: 2.224396587, validation loss: 0.777226985.\n",
      "At epoch 14, training loss: 1.792779939, validation loss: 0.618356489.\n",
      "At epoch 15, training loss: 1.409137154, validation loss: 0.477096486.\n",
      "At epoch 16, training loss: 1.068248263, validation loss: 0.351531254.\n",
      "At epoch 17, training loss: 0.765461330, validation loss: 0.239956650.\n",
      "At epoch 18, training loss: 0.496617803, validation loss: 0.140850667.\n",
      "At epoch 19, training loss: 0.257996013, validation loss: 0.052851556.\n",
      "At epoch 20, training loss: 0.046265440, validation loss: -0.025259512.\n"
     ]
    }
   ],
   "source": [
    "''' Update the hyper-parameters (num_epoch, lr, lam, and batch_size) according to your observation \n",
    "    to achieve better performance.\n",
    "'''\n",
    "\n",
    "num_epoch = 20\n",
    "lr = 0.69E-2\n",
    "lam = 1E-2\n",
    "batch_size = 128\n",
    "\n",
    "mini_gd_lr = LogisticRegression(vocab_size, num_class, lam)\n",
    "mini_gd_train_losses, mini_gd_valid_losses = mini_batch_gd(mini_gd_lr, x_train, y_train, x_valid, y_valid, batch_size, lr, lam, num_epoch, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152527c",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation\n",
    "You are required to report the precision and recall for each category on test set and plot the training loss and validation loss for both SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cceed",
   "metadata": {},
   "source": [
    "##### Please run the following cell to evaluate your model with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "926eb8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "\n",
      "  Precision:\n",
      "    class 0: 0.0382, class 1: 0.0081, class 2: 0.0172, class 3: 0.0067\n",
      "\n",
      "  Recall:\n",
      "    class 0: 0.0216, class 1: 0.0184, class 2: 0.0158, class 3: 0.0016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_hat = sgd_lr.predict(x_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('SGD')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434a82d",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot training loss and validation loss for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e5816db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm seems to have gotten stuck in a local minimum. Perhapse using learning rate decay might help? Reducing lr and lambda and increasing epochs should improve model fitting.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(range(num_epoch), sgd_train_losses, sgd_valid_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('SGD')\n",
    "plt.show()\n",
    "\n",
    "print('The algorithm seems to have gotten stuck in a local minimum. Perhapse using learning rate decay might help? Reducing lr and lambda and increasing epochs should improve model fitting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29063c6f",
   "metadata": {},
   "source": [
    "##### Please run the following cell to evaluate your model with Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c4d23aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD\n",
      "\n",
      "  Precision:\n",
      "    class 0: 0.0330, class 1: 0.0064, class 2: 0.0232, class 3: 0.0148\n",
      "\n",
      "  Recall:\n",
      "    class 0: 0.0437, class 1: 0.0089, class 2: 0.0279, class 3: 0.0011\n"
     ]
    }
   ],
   "source": [
    "y_hat = mini_gd_lr.predict(x_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('Mini-batch GD')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7babf",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot training loss and validation loss for Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ba7d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epoch), mini_gd_train_losses, mini_gd_valid_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('Mini-batch GD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70490a3",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation (10 points)\n",
    "\n",
    "You are required to implement cross-validation, and use it to choose the best $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e1a9a",
   "metadata": {},
   "source": [
    "### 3.1 Reload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82c313b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainc = train_df['text'].values.astype(str)\n",
    "label_trainc = train_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "text_testc = test_df['text'].values.astype(str)\n",
    "label_testc = test_df['label'].values.astype(int) - 1 # -1 because labels start from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59485ce",
   "metadata": {},
   "source": [
    "### 3.2 Define the range of $\\lambda$. (Fill the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7771dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1E-2,1E-4,1E-6,1E-8] ## Fill the values of lambda you want to evaluate in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8d6c3",
   "metadata": {},
   "source": [
    "### 3.3 Implement cross-validation. (Fill the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d0ba8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fe36cad09f45eb8da6d3d3439cfa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c938b200316e43beb070d69985d47062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bc6d23a4d646aba2f30f6be5fb0746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338bbcc2aace4295bfb15beb06d80b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cross val:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.01 , average k-fold cross val loss: 19.054154536954492\n",
      "Lambda: 0.0001 , average k-fold cross val loss: 18.360750120268676\n",
      "Lambda: 1e-06 , average k-fold cross val loss: 19.334570313460645\n",
      "Lambda: 1e-08 , average k-fold cross val loss: 19.355079816036394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "#from tqdm import trange\n",
    "\n",
    "#vocab_size = 10\n",
    "\n",
    "extractor = TfIdfExtractor(vocab_size=vocab_size)\n",
    "extractor.fit(text_trainc)\n",
    "\n",
    "x_trainc = extractor.transform(text_trainc)\n",
    "x_testc = extractor.transform(text_testc)\n",
    "\n",
    "\n",
    "# convert label to one-hot vector\n",
    "y_trainc = np.zeros((label_trainc.shape[0], num_class))\n",
    "y_trainc[np.arange(label_trainc.shape[0]), label_trainc] = 1\n",
    "\n",
    "y_testc = np.zeros((label_testc.shape[0], num_class))\n",
    "y_testc[np.arange(label_testc.shape[0]), label_testc] = 1\n",
    "\n",
    "x_trainc, y_trainc = shuffle(x_trainc,y_trainc)\n",
    "\n",
    "trainc_size = x_trainc.shape[0]\n",
    "#print('train size =',train_size)\n",
    "k = 10\n",
    "lr = 1E-4\n",
    "batch_size = int(trainc_size/k)\n",
    "#print('batch size =',batch_size)\n",
    "eps = 5\n",
    "losses = []\n",
    "\n",
    "for lam in tqdm(lambdas, desc='cross val'):\n",
    "    loss = 0\n",
    "    model = LogisticRegression(vocab_size, num_class, lam)\n",
    "    for i in range(k):\n",
    "        xv = x_trainc[i*batch_size:(i+1)*batch_size]\n",
    "        yv = y_trainc[i*batch_size:(i+1)*batch_size]\n",
    "        xt = np.concatenate([x_trainc[:i*batch_size],x_trainc[(i+1)*batch_size:]])\n",
    "        yt = np.concatenate([y_trainc[:i*batch_size],y_trainc[(i+1)*batch_size:]])\n",
    "        mini_gd_train_losses, mini_gd_valid_losses = mini_batch_gd(model, xt, yt, xv, yv, batch_size, lr, lam, eps)\n",
    "        if loss == 0:\n",
    "                loss = mini_gd_valid_losses[-1]\n",
    "        else:\n",
    "                loss = (loss + mini_gd_valid_losses[-1]) / 2\n",
    "    losses.append(loss)\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    print('Lambda:', lambdas[i],', average k-fold cross val loss:', losses[i])\n",
    "\n",
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c6470817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda value was 1E-4\n",
      "Mini-batch GD\n",
      "\n",
      "  Precision:\n",
      "    class 0: 0.2671, class 1: 0.2433, class 2: 0.2490, class 3: 0.2680\n",
      "\n",
      "  Recall:\n",
      "    class 0: 0.2405, class 1: 0.2289, class 2: 0.2547, class 3: 0.3042\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "lam = 1E-4\n",
    "print('best lambda value was 1E-4')\n",
    "mini_gd_lr = LogisticRegression(vocab_size, num_class, 1E-4)\n",
    "mini_gd_train_losses, mini_gd_valid_losses = mini_batch_gd(mini_gd_lr, x_trainc, y_trainc, x_testc, y_testc, batch_size, lr, 1E-4, num_epoch)\n",
    "\n",
    "y_hat = mini_gd_lr.predict(x_test)\n",
    "y_true = np.argmax(y_testc, axis=1)\n",
    "\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('Mini-batch GD')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2b497",
   "metadata": {},
   "source": [
    "### 3.4 Report the best $lambda$ value, and report the recall and precision for each category on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3acea6",
   "metadata": {},
   "source": [
    "From the tested lambda values, the best one w.r.t. the other fixed hyperparameters seems to be 0.0001.\n",
    "The precision and recall results were found to be as following: (note the model was evaluated using minibatch_gd with batch size = train_size/10)\n",
    "\n",
    "  Precision:\n",
    "    class 0: 0.2671, class 1: 0.2433, class 2: 0.2490, class 3: 0.2680\n",
    "\n",
    "  Recall:\n",
    "    class 0: 0.2405, class 1: 0.2289, class 2: 0.2547, class 3: 0.3042"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ceaeac",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "\n",
    "provide an analysis for the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c93ce",
   "metadata": {},
   "source": [
    "From the above experement, it seems that sgd is much harder and computationally intensive way to optimize the model over the loss landscape compared to minibatch gd. Is this because we average the losses and in turn are \"smoothening\" the loss landscape?\n",
    "Anothe interesting thing that is seen is that for a decently large minibatch size, the regularization factor lambda seems to produce the best result when it is close to the value of learning rate alpha. Intuitively, having a slightly smaller alpha and lambda value with more epochs should result in better optimization of the model's parameters.\n",
    "Due to time and computational constraints, I limited my vocab size to 1000 and ran the code for a max of 20 epochs. I believe the model would yield better results if the hyper parameters are optimized better using k-fold cross val over larger cross val epochs. I used 5 eps for this implementation but in the real world, I would prefer to run cross val for 10-20 eps depending on model complexity.\n",
    "Overall, the model seems to be performing better after cross validation of lambda value wrt other fixed hyperparameters. Another way to estimate the best hypermarater combination might be to loop over various combinations of these hyper parameters and pic the best perfoming combination. In any case, the precision and recall scores have improved. But they are no way near perfect for real world use. Increasing the vocab size might help to a certain extent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
