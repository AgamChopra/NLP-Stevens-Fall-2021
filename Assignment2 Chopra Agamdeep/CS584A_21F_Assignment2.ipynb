{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ccdc559",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Agamdeep S. Chopra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9987d7",
   "metadata": {},
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement MLP.\n",
    "3. Implement Word2Vec\n",
    "\n",
    "*** Please read the code and comments very carefully and install these packages (NumPy, Pandas, sklearn, tqdm, spacy, and matplotlib) before you start ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9720b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (1.20.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (4.60.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from matplotlib) (2021.5.30)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.1.3-cp38-cp38-win_amd64.whl (12.0 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp38-cp38-win_amd64.whl (451 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.5-cp38-cp38-win_amd64.whl (112 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy) (20.9)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.5-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Using cached blis-0.7.4-cp38-cp38-win_amd64.whl (6.5 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy) (4.60.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy) (1.20.2)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.5-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-legacy, pathy, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 cymem-2.0.5 murmurhash-1.0.5 pathy-0.6.0 preshed-3.0.5 pydantic-1.8.2 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0 wasabi-0.8.2\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.60.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\agama\\anaconda3\\envs\\ai\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                                                             #\n",
    "#    Run this cell to make sure all packages are installed.   #\n",
    "#                                                             #\n",
    "###############################################################\n",
    "\n",
    "!pip install numpy pandas scikit-learn tqdm matplotlib\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7df6e9",
   "metadata": {},
   "source": [
    "## 1. MLP (30 points)\n",
    "In this section, you are required to implement the MLP in the following steps.\n",
    "1. Data Processing (Same as assignment 1, you could directly use your code in assignment 1.)\n",
    "2. MLP\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23cf27",
   "metadata": {},
   "source": [
    "### 1.1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33538884",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ec5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# training data\n",
    "train_df = pd.read_csv('./data/train.csv', header=None)\n",
    "train_df.columns = ['label', 'title', 'text']\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('./data/test.csv', header=None)\n",
    "test_df.columns = ['label', 'title', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24ca05",
   "metadata": {},
   "source": [
    "####  Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0e3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocesser(object):\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "    \n",
    "    def apply(self, text):\n",
    "        if self.url:\n",
    "            text = self._remove_url(text)\n",
    "            \n",
    "        if self.punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "            \n",
    "        if self.number:\n",
    "            text = self._remove_number(text)\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "        \n",
    "    def _remove_punctuation(self, text):\n",
    "        ''' Please fill this function to remove all the punctuations in the text\n",
    "        '''\n",
    "        return text.translate(str.maketrans('','', string.punctuation))\n",
    "    \n",
    "    def _remove_url(self, text):\n",
    "        ''' Please fill this function to remove all the urls in the text\n",
    "        '''\n",
    "        return re.sub(r'http\\S+','',text)\n",
    "    \n",
    "    def _remove_number(self, text):\n",
    "        ''' Please fill this function to remove all the numbers in the text\n",
    "        '''\n",
    "        return text.translate(str.maketrans('','', string.digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96430233",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7872df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    ''' Please fill this function to tokenize text.\n",
    "            1. Tokenize the text.\n",
    "            2. Remove stop words.\n",
    "            3. Optional: lemmatize words accordingly.\n",
    "    '''\n",
    "    \n",
    "    ### Start your code\n",
    "    processer = Preprocesser()\n",
    "    clean_text = processer.apply(text)\n",
    "    tokens = clean_text.split()  \n",
    "    ### End\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21083a6f",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724783e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: 96000\n",
      "The size of validation set: 24000\n",
      "The size of testing set: 7600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train = train_df['text'].values.astype(str)\n",
    "label_train = train_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "text_test = test_df['text'].values.astype(str)\n",
    "label_test = test_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "\n",
    "text_train, text_valid, label_train, label_valid = train_test_split(text_train, label_train, test_size=0.2)\n",
    "\n",
    "\n",
    "print('The size of training set:', text_train.shape[0])\n",
    "print('The size of validation set:', text_valid.shape[0])\n",
    "print('The size of testing set:', text_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556119ac",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbed65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class TfIdfExtractor(object):\n",
    "    \n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab = defaultdict(lambda: 0)\n",
    "        self.word2idx = {}\n",
    "        self.df = defaultdict(lambda: 0)\n",
    "        self.num_doc = 0\n",
    "        \n",
    "        self.processer = Preprocesser()\n",
    "        \n",
    "        \n",
    "    def fit(self, texts):\n",
    "        ''' In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary (self.vocab).\n",
    "                2. Construct the document frequency dictionary (self.df).\n",
    "                3. Sort the vocabulary based on the frequence (self.vocab).\n",
    "            Input:\n",
    "                texts: a list of text (training set)\n",
    "            Output:\n",
    "                None\n",
    "        '''\n",
    "\n",
    "        self.num_doc = len(texts)\n",
    "        \n",
    "        for text in tqdm(texts, desc='fitting text'):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)\n",
    "            \n",
    "            ### Start your code (step 1 & 2)\n",
    "            for i in tokens:\n",
    "                self.vocab[i]\n",
    "                self.df[i] += 1\n",
    "            ### End\n",
    "        \n",
    "        ### Start your code (Step 3)\n",
    "        self.vocab = dict(sorted(self.vocab.items(), key=lambda x: self.df[x[0]], reverse=True))\n",
    "        #print(self.vocab)\n",
    "        ### End\n",
    "        \n",
    "        if self.vocab_size is not None:\n",
    "            self.vocab = {key: self.vocab[key] for key in list(self.vocab.keys())[:self.vocab_size]}\n",
    "        \n",
    "        self.word2idx = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
    "        \n",
    "        #print(self.vocab)\n",
    "        #print(self.word2idx)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        ''' In this function, you need to encode the input text into TF-IDF vector.\n",
    "            Input:\n",
    "                texts: a list of text.\n",
    "            Ouput:\n",
    "                a N-d matrix (Tf-Idf) \n",
    "        '''\n",
    "        tfidf = np.zeros((len(texts), len(self.vocab)))\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), desc='transforming', total=len(texts)):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)      \n",
    "            tfidf[i,:] = [tokens.count(list(self.vocab.keys())[j]) for j in range(len(self.vocab))]\n",
    "\n",
    "        tfidf = np.log(1+tfidf)*np.log(1+(len(texts))/np.array([self.df[list(self.vocab.keys())[i]] for i in range(len(self.vocab))]))\n",
    "\n",
    "        '''\n",
    "        \n",
    "        def _tf_idf(self, token, tokens):\n",
    "        tf = tokens.count(token) / len(tokens)\n",
    "        idf = np.log(self.num_doc / self.df[token])\n",
    "        return tf * idf\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f27ddb",
   "metadata": {},
   "source": [
    "#### Obtain the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f8e2d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3368dd87379e49c09f349103a5ed26f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/96000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9bb70cf72d4e7ba62ea5982b1abeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/96000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b05c563f614c88baef862a5faa3b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f47e646dc845cfaced09eb8f41f669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: (96000, 400)\n",
      "The size of validation set: (24000, 400)\n",
      "The size of test set: (7600, 400)\n"
     ]
    }
   ],
   "source": [
    "# You can change this number to see the difference of the performances. (larger vocab size needs more memory)\n",
    "vocab_size = 400#0 \n",
    "num_class = 4\n",
    "\n",
    "extractor = TfIdfExtractor(vocab_size=vocab_size)\n",
    "extractor.fit(text_train)\n",
    "\n",
    "x_train = extractor.transform(text_train)\n",
    "x_valid = extractor.transform(text_valid)\n",
    "x_test = extractor.transform(text_test)\n",
    "\n",
    "\n",
    "# convert label to one-hot vector\n",
    "y_train = np.zeros((label_train.shape[0], num_class))\n",
    "y_train[np.arange(label_train.shape[0]), label_train] = 1\n",
    "\n",
    "y_valid = np.zeros((label_valid.shape[0], num_class))\n",
    "y_valid[np.arange(label_valid.shape[0]), label_valid] = 1\n",
    "\n",
    "y_test = np.zeros((label_test.shape[0], num_class))\n",
    "y_test[np.arange(label_test.shape[0]), label_test] = 1\n",
    "\n",
    "\n",
    "print('The size of training set:', x_train.shape)\n",
    "print('The size of validation set:', x_valid.shape)\n",
    "print('The size of test set:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57135412",
   "metadata": {},
   "source": [
    "### 1.2 MLP (30 points)\n",
    "\n",
    "In this section, you are required to implement a 1-layer MLP from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d67bdd",
   "metadata": {},
   "source": [
    "#### 1.2.1 Implement MLP (fill the code: 20 points)\n",
    "\n",
    "> $z_1 = w_1x$\n",
    "\n",
    "> $h_1 = activation(z_1)$\n",
    "\n",
    "> $z_2 = w_2 h_1$\n",
    "\n",
    "> $\\hat{y} = softmax(z_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3ff8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, num_feature, hidden_size, num_class):\n",
    "        ''' Initialize the weight of MLP.\n",
    "            Inputs:\n",
    "                num_feature: scalar, the number of features (in this case, it is the vocab size).\n",
    "                hidden_size: scaler, the number of neurons in the hidden layer.\n",
    "                num_class: scalar, the number of classes.\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        self.w1 = np.random.rand(num_feature, hidden_size)\n",
    "        self.grad_w1 = np.zeros(self.w1.size)\n",
    "        self.w2 = np.random.rand(hidden_size, num_class)\n",
    "        self.grad_w2 = np.zeros(self.w2.size)\n",
    "        ### End\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Implement the forward pass.\n",
    "            Input:\n",
    "                x: N-d matrix\n",
    "            Outputs\n",
    "                y_hat: the output of the model, N-K matrix.\n",
    "                h1: the output of the first hidden layer.\n",
    "                z1: the output of the first hidden before activation function.\n",
    "                \n",
    "                Note that the reason for return h1 and z1 is for calculating the gradient of self.w1 and self.w2.\n",
    "                Feel free to change it accordingly.\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "\n",
    "        h1 = np.dot(x,self.w1)\n",
    "        z1 = self.activation(h1)\n",
    "        #print('z1',z1.shape)\n",
    "        y_hat = self.softmax(np.dot(z1,self.w2)).squeeze()\n",
    "        #print('y_hat',y_hat.shape)\n",
    "        ### End\n",
    "        \n",
    "        return y_hat, (h1, z1)\n",
    "    \n",
    "    \n",
    "    def backward(self, lr, x, y, y_hat, h1, z1):\n",
    "        ''' Implement back-propagation.\n",
    "            Inputs:\n",
    "                lr: learning rate.\n",
    "                x: the input, N-d matrix.\n",
    "                y_hat: the output, N-K matrix.\n",
    "                y: ground truth (N-K one-hot matrix).\n",
    "                h1: the output of the first hidden layer.\n",
    "                z1: the output of the first hidden before activation function.\n",
    "        '''\n",
    "\n",
    "        # Get the gradient of w1 and w2\n",
    "        self.grad_w1, self.grad_w2 = self.gradient(x, y, y_hat, h1, z1)\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.w1 -= lr * self.grad_w1\n",
    "        self.w2 -= lr * self.grad_w2\n",
    "        \n",
    "    \n",
    "    def objective(self, y, y_hat):\n",
    "        ''' Compute the loss\n",
    "            Inputs:\n",
    "                y: N-K matrix, ground truth.\n",
    "                y_hat: N-K matrix, prediction.\n",
    "            Output:\n",
    "                loss: scalar, the loss of the model.\n",
    "        '''\n",
    "        \n",
    "        loss = 0.\n",
    "        \n",
    "        ### Start your code\n",
    "        loss = -np.mean(np.sum(y * np.log(y_hat),axis=1))\n",
    "        ### End\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def gradient(self, x, y, y_hat, h1, z1):\n",
    "        ''' Compute the gradient of self.w1 and self.w2\n",
    "            Inputs:\n",
    "                x: the input, N-d matrix.\n",
    "                y_hat: the output, N-K matrix.\n",
    "                y: ground truth (N-K one-hot matrix).\n",
    "                h1: the output of the first hidden layer.\n",
    "                z1: the output of the first hidden before activation function.\n",
    "            Outputs:\n",
    "                grad_w1: the gradient of self.w1.\n",
    "                grad_w2: the gradient of self.w2.\n",
    "        '''\n",
    "        n = x.shape[0]\n",
    "        \n",
    "        ### Start your code\n",
    "        #print('y', y.shape)\n",
    "        self.grad_w2 = (1/n)*np.dot(z1.T,(y_hat - y))\n",
    "        #print('grad_w2',self.w2.shape)\n",
    "        self.grad_w1 = (1/n)*np.dot(x.T,((z1*(1 - z1))*np.dot((y_hat - y),self.w2.T)))\n",
    "        #print('grad_w1',self.w1.shape)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return self.grad_w1, self.grad_w2\n",
    "    \n",
    "    \n",
    "    def activation(self, x):\n",
    "        ''' Implement an activation function, ReLU or Sigmoid.\n",
    "            Please note that for different activation functions, \n",
    "            you have to implement different gradient in function \"backward()\"\n",
    "            \n",
    "            Input:\n",
    "                x: N-d matrix\n",
    "            Output:\n",
    "                x: sigmoid(x) or ReLU(x)\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        x = 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x - np.max(x)\n",
    "            return np.exp(x) / np.sum(np.exp(x))\n",
    "        else:\n",
    "            x = x - np.max(x, axis=1).reshape(-1, 1)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90f5d5",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "Please run the following cell to do the gradient checking. This block will check your implementation of calculating the gradient of w1 and w2. Make sure you pass this test, otherwise you can not train your model correctly.\n",
    "\n",
    "**Note that** if you don't pass this test case, please check the functions **\"MLP.gradient()\"** and **\"MLP.objective()\"** very carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39db57d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#                                             #\n",
    "#   Checking functions below. DO NOT MODIFY   #\n",
    "#                                             #\n",
    "###############################################\n",
    "def gradient_checking(model, X, y, epsilon):    \n",
    "    y_hat, (h1, z1) = model.forward(X)\n",
    "    grad_w1, grad_w2 = model.gradient(X, y, y_hat, h1, z1)\n",
    "    \n",
    "    it = np.nditer(model.w2, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        model.w2[ix] += epsilon # increment by eps\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l1 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w2[ix] -= 2 * epsilon # restore to previous value (very important!)\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l2 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w2[ix] += epsilon\n",
    "        numgrad = (l1 - l2) / 2 / epsilon\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad_w2[ix])\n",
    "        if reldiff > 1e-4:\n",
    "            print(\"Gradient check failed for w2.\")\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad_w2[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "        \n",
    "    it = np.nditer(model.w1, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        model.w1[ix] += epsilon # increment by eps\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l1 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w1[ix] -= 2 * epsilon # restore to previous value (very important!)\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l2 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w1[ix] += epsilon\n",
    "        numgrad = (l1 - l2) / 2 / epsilon\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad_w1[ix])\n",
    "        if reldiff > 1e-4:\n",
    "            print(\"Gradient check failed for w1.\")\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad_w1[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "        \n",
    "    print(\"Gradient check passed!\")\n",
    "    \n",
    "epsilon = 1E-4\n",
    "np.random.seed(10)\n",
    "dummy_X = np.random.rand(10, 5)\n",
    "dummy_y = np.eye(3)[np.random.choice(3, 10)]\n",
    "dummy_model = MLP(5, 3, 3)\n",
    "gradient_checking(dummy_model, dummy_X, dummy_y, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a59617",
   "metadata": {},
   "source": [
    "#### 1.2.2 Optimization (Fill the code: 10 points)\n",
    "\n",
    "In this section, you are requried to call the MLP model to implement the Mini-batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b0b03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(model, X, y, lr, batch_size=None, num_epoch=100):\n",
    "    ''' Implement Mini-batch GD\n",
    "        Inputs:\n",
    "            X: N-d matrix\n",
    "            y: N vector\n",
    "            lr: learning rate\n",
    "            batch_size: optional, depends on if you use Mini-batch GD\n",
    "            num_epoch: the number of epochs\n",
    "        Output:\n",
    "            A list of training losses against epoch\n",
    "            A list of validation losses against epoch\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        rand_indices = np.random.permutation(n)\n",
    "        x_rand = X[rand_indices]\n",
    "        y_rand = y[rand_indices]\n",
    "        \n",
    "        for b in tqdm(range(0, n, batch_size), f'Epoch {e+1}/{num_epoch}'):\n",
    "            x_batch = x_rand[b: b + batch_size]\n",
    "            y_batch = y_rand[b: b + batch_size]\n",
    "            \n",
    "            ### Start your code\n",
    "            \n",
    "            ### Step 1, call forward function to get outputs\n",
    "\n",
    "            y_hat, (h1, z1) = model.forward(x_batch)\n",
    "            ### Step 2, call objective function to get loss\n",
    "\n",
    "            loss = model.objective(y_batch, y_hat)\n",
    "            ### Step 3, call backward to update the weights\n",
    "\n",
    "            model.backward(lr, x_batch, y_batch, y_hat, h1, z1)\n",
    "            # End\n",
    "            \n",
    "            losses.append(loss)\n",
    "        \n",
    "        train_loss = np.mean(losses)\n",
    "        \n",
    "        \n",
    "        y_hat, _ = model.forward(x_valid)\n",
    "        valid_loss = model.objective(y_valid, y_hat)\n",
    "        \n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f'At epoch {e+1}, training loss: {train_loss:.6f}, validation loss: {valid_loss:.6f}.')\n",
    "            \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c4483",
   "metadata": {},
   "source": [
    "#### Run the following cell to train the model. (Feel free to tune the hpyer-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91e6e13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168f9580c274409da3b550d2d9e458a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1, training loss: 1.394810, validation loss: 1.391213.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f1dd9dc0604867ad8eb5be92cf5fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 2, training loss: 1.392371, validation loss: 1.389015.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0201e062dfb430f8bf3fa6c9ca6145d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 3, training loss: 1.392156, validation loss: 1.391259.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071d9694c9854e25b9bfe215dbb5d2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 4, training loss: 1.392674, validation loss: 1.409447.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b01ac1cbdaf4af19a8e445c3d01c500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 5, training loss: 1.391988, validation loss: 1.392768.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7d33b50d114c6da38a581d8c84de8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 6, training loss: 1.391974, validation loss: 1.389567.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a804546df64095a939efd22e071d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 7, training loss: 1.391790, validation loss: 1.394144.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bda2b1c38674304a23a0b77c805c2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 8, training loss: 1.391765, validation loss: 1.385583.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687b83c7a13424d849ddda308742b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 9, training loss: 1.391466, validation loss: 1.390374.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818f9794d11844fdab9663f6af054f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 10, training loss: 1.391571, validation loss: 1.393601.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c5edea7612472a9ab7a6a7ca983d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 11, training loss: 1.390986, validation loss: 1.391810.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53c3d531e31474a9bd4f6dd5fc98679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 12, training loss: 1.391020, validation loss: 1.384369.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a61094a727c447c9f4cdc99a37b2e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 13, training loss: 1.391005, validation loss: 1.389199.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309fd4e6148e4f7489662a4a196be6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 14, training loss: 1.390760, validation loss: 1.396266.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f918fcffc4e485f81b73ca9879ff70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 15, training loss: 1.390609, validation loss: 1.387096.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a4ce1b5caf4efd936c2757c754bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 16, training loss: 1.390555, validation loss: 1.386980.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113b6d2814ea46a2861d5558aad5b317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 17, training loss: 1.390153, validation loss: 1.388335.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee481c5802f4f69baa8c37c6a4cf145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 18, training loss: 1.389547, validation loss: 1.389517.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7050528691b74d6db92c4d7586d36da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 19, training loss: 1.389910, validation loss: 1.383974.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72845bb86d624195a62e981c1945b89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 20, training loss: 1.389718, validation loss: 1.387661.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7d540ccac04b3c87ecbab2606ee5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 21, training loss: 1.389501, validation loss: 1.384263.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e08e243e56a422689a65cc66273a423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 22, training loss: 1.389204, validation loss: 1.381434.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46740eb9aabb4891b288152ca81ad634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 23, training loss: 1.389354, validation loss: 1.381559.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cedd3edc99549e3997eb755c5405269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 24, training loss: 1.388877, validation loss: 1.379571.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cc85952b9b42f0870afd923787f9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 25, training loss: 1.388539, validation loss: 1.380748.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be98258305eb4587a110ad005fa5bcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 26, training loss: 1.388863, validation loss: 1.380019.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ea2e3c2cf4ea8b2af90e09eaecce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 27, training loss: 1.387699, validation loss: 1.386972.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d443b1b7ae48c8a2c5883c48003b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 28, training loss: 1.387680, validation loss: 1.381220.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cd8a20a636403ea94e488eb07fb986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 29, training loss: 1.387677, validation loss: 1.377174.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f253627d54444f87baa9bcb7f580944a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 30, training loss: 1.387246, validation loss: 1.398796.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64c5b6805804b91b2806af4b2a7f47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 31, training loss: 1.386590, validation loss: 1.379292.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df9737827d8414297a61330301a798f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 32/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 32, training loss: 1.386298, validation loss: 1.380597.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfa34a1d3c8424ab721398a47f75e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 33/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 33, training loss: 1.385782, validation loss: 1.380389.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92988d1ddfde4fdf8c9340a85d7b0f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 34/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 34, training loss: 1.384862, validation loss: 1.377412.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a3f30e6c9c422295ed6b5cd3bf3734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 35/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 35, training loss: 1.384544, validation loss: 1.372966.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef428482e2f444d95e193450de392f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 36/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 36, training loss: 1.383064, validation loss: 1.370788.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a22290cb374ad28a439607bdc4a6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 37/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 37, training loss: 1.381937, validation loss: 1.367480.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205792cf82644251b5fd8d1562ebc922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 38/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 38, training loss: 1.379772, validation loss: 1.366817.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9056bdfadd474c44aca5e7ef2616c5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 39/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 39, training loss: 1.376021, validation loss: 1.360175.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2e6d31f7414612bf10c953b8741009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 40/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 40, training loss: 1.368676, validation loss: 1.352127.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909923583aea4ea39b246aef5a91eb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 41/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 41, training loss: 1.355601, validation loss: 1.337216.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cec62c942f74545bb450fb2a4b371d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 42/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 42, training loss: 1.335278, validation loss: 1.315822.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e573b667b5b4f21a4bb468be645fb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 43/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 43, training loss: 1.307683, validation loss: 1.291618.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae63a0dad4847cabf81ba8b836edc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 44/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 44, training loss: 1.276749, validation loss: 1.250831.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a845433a81084e5fb53d9aedef4561fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 45/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 45, training loss: 1.243675, validation loss: 1.218535.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16b3704f73842c3b67a4daf22746c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 46/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 46, training loss: 1.209623, validation loss: 1.192331.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66058078aa864dfaa56ccd3340c86593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 47/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 47, training loss: 1.171775, validation loss: 1.143040.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d768158a6240d7ba935fb430517091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 48/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 48, training loss: 1.127327, validation loss: 1.092354.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb49ebbb1a84a51a642e3c92524321b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 49/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 49, training loss: 1.080637, validation loss: 1.048308.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdc1ac6b5d44a91b7c54a991e39ded9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 50/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 50, training loss: 1.046102, validation loss: 1.021684.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2f8ed0069c4cbb9172fa37ac7f8820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 51/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 51, training loss: 1.021857, validation loss: 1.003869.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4323450e0b4888a97311e077400dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 52/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 52, training loss: 1.003190, validation loss: 0.987422.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27463c842ec84623ae1b9a3baaa6fbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 53/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 53, training loss: 0.988909, validation loss: 0.976231.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97304d808d0e42deac00041145f5324a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 54/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 54, training loss: 0.978189, validation loss: 0.967970.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1646716cba3c43ed9a9183bcafe88554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 55/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 55, training loss: 0.970459, validation loss: 0.962986.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69467cdffc24f40b1328e9c9aff8efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 56/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 56, training loss: 0.964131, validation loss: 0.956614.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af56332bd7b427e9e870f09dea9b511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 57/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 57, training loss: 0.959182, validation loss: 0.955697.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a34c66d68b4dd995bc1101a53801a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 58/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 58, training loss: 0.954111, validation loss: 0.947498.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cec58099a048bcaeecdd95078ed064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 59/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 59, training loss: 0.948411, validation loss: 0.944684.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccc7e4f5c7d4c07ac2ff5251b61d3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 60/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 60, training loss: 0.941522, validation loss: 0.934632.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0683de022ed421dabb30a3543d0df78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 61/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 61, training loss: 0.932408, validation loss: 0.932525.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126918a6bbde43439852beb47e18c091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 62/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 62, training loss: 0.922016, validation loss: 0.913995.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66adb3e5b6c425c9b045bf3568f2eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 63/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 63, training loss: 0.909672, validation loss: 0.905417.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0a876549e245e5a1c012fa7cc50564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 64/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 64, training loss: 0.895891, validation loss: 0.887520.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce7a68b05f34f8aa6b5803e455f7d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 65/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 65, training loss: 0.880411, validation loss: 0.876081.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3c185377854a0183ac398d8ed6f96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 66/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 66, training loss: 0.863685, validation loss: 0.859064.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c3e0e44d024b21bb83b0dd4e732e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 67/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 67, training loss: 0.845940, validation loss: 0.844830.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00514e8e2655450a9bcff88387d0047e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 68/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 68, training loss: 0.822049, validation loss: 0.817597.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d53f18332c407aa06a4a1f52bbbdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 69/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 69, training loss: 0.796015, validation loss: 0.798198.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2648b257599407f8e5f04d513d75fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 70/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 70, training loss: 0.773395, validation loss: 0.776510.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4973c8debdcc447e8984ef70a4036957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 71/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 71, training loss: 0.755044, validation loss: 0.784096.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a5ffda08b94f998d660c0b0903b5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 72/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 72, training loss: 0.739344, validation loss: 0.754663.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab246853e0048b18a8507bc96f807db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 73/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 73, training loss: 0.725452, validation loss: 0.736672.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef71a05e58746f88d2d5eea2d97d95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 74/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 74, training loss: 0.711672, validation loss: 0.733555.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef3485232cf43f0b6b0910ed43ac84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 75/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 75, training loss: 0.698280, validation loss: 0.701299.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1491acca6446469a5a8e6a8001a6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 76/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 76, training loss: 0.685389, validation loss: 0.706370.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b8015471a648a2b6b8f017fd008ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 77/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 77, training loss: 0.672896, validation loss: 0.696528.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c6302273b644a09f6e27a98ca0670d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 78/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 78, training loss: 0.660948, validation loss: 0.704042.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c558b3f06942bd9d0ec1fac376b4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 79/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 79, training loss: 0.651666, validation loss: 0.672013.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23937c15964410cbb47717ba5fa325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 80/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 80, training loss: 0.644648, validation loss: 0.688686.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bd0af5bbf64bee9e94c777783d70e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 81/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 81, training loss: 0.639501, validation loss: 0.686985.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2273cd78e34a818bd2cab61e9e4beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 82/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 82, training loss: 0.635718, validation loss: 0.655547.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22433b1ea0c4e999cba5640438608ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 83/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 83, training loss: 0.632369, validation loss: 0.646468.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6214e0aa9654929896c976ca0833ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 84/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 84, training loss: 0.630130, validation loss: 0.654432.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d370a75dd0408ebb26b8f6a6c2a77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 85/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 85, training loss: 0.628152, validation loss: 0.652851.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4c8fade74a4b6191c86f422cff36b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 86/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 86, training loss: 0.626469, validation loss: 0.639677.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cd9ef1b2934ebc9c6d48c872e6a300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 87/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 87, training loss: 0.625333, validation loss: 0.649636.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb08b30c41c48abbf29051679922d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 88/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 88, training loss: 0.624103, validation loss: 0.645696.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97afd0b1a74343dc830d1bed3bfe80ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 89/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 89, training loss: 0.623169, validation loss: 0.657617.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e38c5c5483541ab9cf0d1bfb74c3d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 90/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 90, training loss: 0.622119, validation loss: 0.643807.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261eedbf8e51474d8a52bc4e5f4ecff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 91/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 91, training loss: 0.621450, validation loss: 0.640491.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20721ec0b6d1493ebf95e2ea49b9c2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 92/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 92, training loss: 0.620678, validation loss: 0.645303.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b99104df4d4cd09a1cf50903a75c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 93/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 93, training loss: 0.620074, validation loss: 0.649406.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27401165c8fe435fa08e9afd79508647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 94/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 94, training loss: 0.619496, validation loss: 0.651295.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c430dd406d24fe2b71eebb383b45cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 95/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 95, training loss: 0.619008, validation loss: 0.649726.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637eecc36842491988c4caea70d3018b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 96/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 96, training loss: 0.618628, validation loss: 0.651152.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0559fd6953214b7d811268de51576114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 97/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 97, training loss: 0.618141, validation loss: 0.650537.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6395d8c43d4c4ec88b8d6c783d651bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 98/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 98, training loss: 0.617499, validation loss: 0.650656.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e48a6c9965d44cea7a184f46a07b9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 99/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 99, training loss: 0.617283, validation loss: 0.638412.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8230a451072448438be305283dbab7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 100/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 100, training loss: 0.617190, validation loss: 0.636803.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda627f5777f410f8be49055ba056ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 101/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 101, training loss: 0.616705, validation loss: 0.649425.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbde99b7dc04527884f1d1903a862af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 102/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 102, training loss: 0.616424, validation loss: 0.637109.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f08bff329b2498390d4be2df570f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 103/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 103, training loss: 0.616047, validation loss: 0.651480.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18cd233963244c2a8b58174fe6daf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 104/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 104, training loss: 0.615912, validation loss: 0.650176.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b08935662f4ffc83407579b4832332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 105/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 105, training loss: 0.615386, validation loss: 0.643880.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3657162f0544be48a8afaf37f49e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 106/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 106, training loss: 0.614886, validation loss: 0.634063.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48a4c3293d46fa82e439a7232e9c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 107/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 107, training loss: 0.614967, validation loss: 0.642288.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fcffc3b63442d59cd9f34c8e324b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 108/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 108, training loss: 0.614531, validation loss: 0.639947.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c18d9291a4d4b41a3b972cd12caec3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 109/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 109, training loss: 0.614805, validation loss: 0.639937.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3adad3ffa5545ec9943841c356386cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 110/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 110, training loss: 0.614626, validation loss: 0.639165.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f593cc2b984b3eb1d678eb8c76e99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 111/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 111, training loss: 0.614200, validation loss: 0.640475.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572b2a163caa46cfafe4c5bc1dd2462b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 112/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 112, training loss: 0.613933, validation loss: 0.640620.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd4c94b36a148199acb0ed58568c2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 113/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 113, training loss: 0.613892, validation loss: 0.635778.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f6f47ef2ba4abe9172c6c853dc3fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 114/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 114, training loss: 0.613522, validation loss: 0.640896.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d6e3f2ddcb450bb20d802cf3c6feb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 115/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 115, training loss: 0.613452, validation loss: 0.643474.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac621bd8d3d741e4893bfe431f21d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 116/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 116, training loss: 0.613386, validation loss: 0.639107.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad740c7a9184889bd06c8d6582e8848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 117/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 117, training loss: 0.613108, validation loss: 0.635787.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd937f736dc4ebf99b531cd31b04aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 118/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 118, training loss: 0.612908, validation loss: 0.631722.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9400487096d434e933d144617cfc984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 119/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 119, training loss: 0.612420, validation loss: 0.638341.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310f3cd3ff4f41fc9d1561744a778603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 120/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 120, training loss: 0.612382, validation loss: 0.635417.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd460790f71e4226853026801425a180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 121/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 121, training loss: 0.612471, validation loss: 0.635872.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6683cb15cb345eda2aae31022cdb5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 122/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 122, training loss: 0.612169, validation loss: 0.634015.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e3b085653746ad95b8a0e44b130a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 123/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 123, training loss: 0.611885, validation loss: 0.642756.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8ff0370ceb4616b7174041d800981d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 124/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 124, training loss: 0.612018, validation loss: 0.649769.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de690912c5743509200570276550215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 125/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 125, training loss: 0.611739, validation loss: 0.645183.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaec0dbcda64d318bc02f97bd32322d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 126/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 126, training loss: 0.611462, validation loss: 0.632964.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8911aec35eaf4566a84a2b34eafde835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 127/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 127, training loss: 0.611247, validation loss: 0.643836.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98584f66185048d5a3a0fd9a8eb39f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 128/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 128, training loss: 0.610962, validation loss: 0.645245.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8fad73368845fbb50a4ff9d9cf7a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 129/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 129, training loss: 0.610943, validation loss: 0.641936.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e7de97c91f4b8cae4554fcf9a114e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 130/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 130, training loss: 0.610756, validation loss: 0.645390.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c52fafc957e4df190f164b810727608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 131/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 131, training loss: 0.610939, validation loss: 0.639153.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21916c69ba79446ca8c1bd9f30859c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 132/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 132, training loss: 0.610639, validation loss: 0.638259.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4158f0078ef44c5be5a275cf77ab145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 133/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 133, training loss: 0.610206, validation loss: 0.634767.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc0918aeae044c3b36ee86d532bd9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 134/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 134, training loss: 0.610312, validation loss: 0.632794.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9f3fac56ed4096bad0e526d80b6591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 135/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 135, training loss: 0.610053, validation loss: 0.645360.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a75c3c475346e79ffad343bacab700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 136/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 136, training loss: 0.609659, validation loss: 0.640362.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53116aa4911944c4b7eb4a6509e8a75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 137/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 137, training loss: 0.609572, validation loss: 0.636299.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a0af62c58d4e8082f9a1af193e7248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 138/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 138, training loss: 0.609551, validation loss: 0.641936.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25f0d90fee64a139eb842512bebad38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 139/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 139, training loss: 0.609396, validation loss: 0.636434.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bfa20e86b04b7fb1f9c6e9ab926c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 140/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 140, training loss: 0.609080, validation loss: 0.639251.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979161e041984b1a87571588870e194c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 141/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 141, training loss: 0.609013, validation loss: 0.635505.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab021130ab9947b3922f4fc58fe4ec30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 142/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 142, training loss: 0.608863, validation loss: 0.637733.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483c52502ec8446aaa40c7bc125d6de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 143/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 143, training loss: 0.608514, validation loss: 0.630224.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06c6cf2ee8d409ab2d7f29919d98f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 144/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 144, training loss: 0.608544, validation loss: 0.643870.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23bb12244c74da2845207df9436a41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 145/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 145, training loss: 0.608646, validation loss: 0.628660.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c898064139ef4f67b46b56bec5618b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 146/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 146, training loss: 0.608183, validation loss: 0.630942.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fb6711d7ad4b24a6e51aed0e082aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 147/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 147, training loss: 0.608083, validation loss: 0.636112.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102ee1f099ad460b826594261d29a1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 148/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 148, training loss: 0.607651, validation loss: 0.630302.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6018c0496548d6bba7d6b8761f3e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 149/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 149, training loss: 0.607537, validation loss: 0.640766.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a2ea018b3f4027b430b0d69572fbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 150/150:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 150, training loss: 0.607375, validation loss: 0.638870.\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 150\n",
    "lr = 1E-2\n",
    "batch_size = 16\n",
    "hidden_size = 50\n",
    "\n",
    "model = MLP(vocab_size, hidden_size, num_class)\n",
    "train_losses, valid_losses = optimization(model, x_train, y_train, lr, batch_size=batch_size, num_epoch=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1974408",
   "metadata": {},
   "source": [
    "#### 1.2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf95172",
   "metadata": {},
   "source": [
    "#### Run the following cell to evaluate the performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22fdddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7600,)\n",
      "MLP\n",
      "\n",
      "  Precision:\n",
      "    class 0: 0.5855, class 1: 0.9327, class 2: 0.8230, class 3: 0.6632\n",
      "\n",
      "  Recall:\n",
      "    class 0: 0.8847, class 1: 0.6268, class 2: 0.5874, class 3: 0.7316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_hat, _ = model.forward(x_test)\n",
    "y_hat = np.argmax(y_hat, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(y_true.shape)\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('MLP')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30462b16",
   "metadata": {},
   "source": [
    "#### Run the following cell to plot the training loss and validation loss against epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51564bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(range(num_epoch), train_losses, valid_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('SGD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936e405",
   "metadata": {},
   "source": [
    "## 2. Word Vector (70 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376c828",
   "metadata": {},
   "source": [
    "In this section, you are required to implement a Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8847f",
   "metadata": {},
   "source": [
    "### 2.1 Written Questions (25 points, each question is 5 points. )\n",
    "\n",
    "\n",
    "To better understand the insight of Word2Vec, Please answer the following questions. (You can either answer those questions in this notebook, or submit a pdf with your answers.)\n",
    "\n",
    "1. Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expressions where only $\\sigma(x)$, but not $x$, is present). Assume that the input $x$ is a scalar for this question. Recall, the sigmoid function is:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "2. Assume you are given a predicted word vector $\\mathbf{v}_c$ corresponding to the center word $c$ for skipgram, and the word prediction is made with the **softmax** function，\n",
    "\n",
    "<center>$\\hat{y}_o = p(o|c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{w=1}^W \\exp(\\mathbf{u}_w^\\top\\mathbf{v}_c)}$</center>\n",
    "\n",
    "> where $o$ is the expected word, $w$ denotes the $w$-th word and $\\mathbf{u}_w$ (w = 1, ..., W) are the “output” word vectors for all words in the vocabulary.\n",
    "The cross entropy function is defined as:\n",
    "\n",
    "<center>$J_\\text{CE}(o,\\mathbf{v}_c, U) =CE(\\mathbf{y}, \\hat{\\mathbf{y}})= -\\sum_i y_i \\log(\\hat{y}_i)$</center>\n",
    "\n",
    "> where the gold vector $\\mathbf{y}$ is a one-hot vector, the softmax prediction vector $\\hat{\\mathbf{y}}$ is a\n",
    "probability distribution over the output space, and $U= [u_1, u_2, ..., u_W]$ is the matrix of all the output vectors. \n",
    "Assume cross entropy cost is applied to this prediction, derive the gradients with respect to $\\mathbf{v}_c$.\n",
    "\n",
    "3. Derive gradients for the \"output\" word vector $\\mathbf{u}_w$ (including $\\mathbf{u}_o$) in the previous part.\n",
    "\n",
    "\n",
    "4. Repeat (2) and (3) assuming we are using the negative sampling loss for the predicted vector $\\mathbf{v}_c$. Assume that K negative samples (words) are drawn and they are 1,...,K respectively. For simplicity of notation, assume ($o\\notin \\{1,...,K\\}$). Again for a given word $o$, use $\\mathbf{u}_o$ to denote its output vector. The negative sampling loss function in this case is:\n",
    "$$J_\\text{neg-sample}(o,\\mathbf{v}_c, U) =-\\log(\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)) -\\sum_{k=1}^K \\log(\\sigma(-\\mathbf{u}_k^\\top \\mathbf{v}_c))$$\n",
    "\n",
    "5. Derive gradients for all of the word vectors for skip-gram given the previous parts and given a set of context words $[\\text{word}_{c-m}, . . . , \\text{word}_c, . . . , \\text{word}_{c+m}]$ where $m$ is the\n",
    "context size. Denote the \"input\" and \"output\" word vectors for word $k$ as $\\mathbf{v}_k$ and $\\mathbf{u}_k$ respectively.\n",
    "\n",
    "> _Hint: feel free to use $F(o, \\mathbf{v}_c)$ (where $o$ is the expected word) as a placeholder for the $J_\\text{CE}(o,\\mathbf{v}_c ...)$ or $J_\\text{neg-sample}(o,\\mathbf{v}_c...)$ cost functions in this part -- you’ll see that this is a useful abstraction for the coding part. That is, your solution may contain terms of the form $\\frac{\\partial F(o, \\mathbf{v}_c)}{\\partial ...}$ Recall that for skip-gram, the cost for a context centered around c is:\n",
    "$$\\sum_{-m \\leq j\\leq m, j\\neq0} F(w_{c+j}, \\mathbf{v}_c)$$_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4166917",
   "metadata": {},
   "source": [
    "### 2.2 Coding (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e2696",
   "metadata": {},
   "source": [
    "#### 2.2.1 Sigmoid Function (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71e3073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ''' Compute the sigmoid function.\n",
    "        Inputs:\n",
    "            x: A scalar or numpy array\n",
    "        Outputs:\n",
    "            s: sigmoid(x)\n",
    "    '''\n",
    "    s = 0.\n",
    "    \n",
    "    ### Start your code\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    # s = np.where(x >= 0, 1/(1 + np.exp(-x)), np.exp(x)/(1 + np.exp(x))) # To prevent overflow this implementation might be better. Please uncomment if errors are seen while testing. Source: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    ''' Compute the softmax function for each row of the input x. \n",
    "        It is crucial that this function is optimized for speed \n",
    "        because it will be used frequently in later code. \n",
    "\n",
    "        Inputs:\n",
    "        x: A D dimensional vector or N x D dimensional numpy matrix.\n",
    "        Outputs:\n",
    "        x: You are allowed to modify x in-place\n",
    "    '''\n",
    "    if len(x.shape) == 1:\n",
    "        x = x - np.max(x)\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "    else:\n",
    "        x = x - np.max(x, axis=1).reshape(-1, 1)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdf46f",
   "metadata": {},
   "source": [
    "#### 2.2.2 Word2Vec models with Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943d5ff",
   "metadata": {},
   "source": [
    "#### Naive Softmax loss & gradient function for word2vec models (fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9012802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset):\n",
    "    ''' Implement tha naive softmax loss and gradients between a center word's embedding \n",
    "        and an outside word's embedding. This will be the building block for our word2vec\n",
    "        models.\n",
    "        \n",
    "        Inputs:\n",
    "            centerWordVec: numpy ndarray, center word's embedding (v_c in question 2.1.1).\n",
    "            outsideWordIdx: integer, the index of the outside word (o of u_o in question 2.1.1).\n",
    "            outsideVectors: outside vectors (rows of matrix) for all words in vocab (U in question 2.1.1).\n",
    "            dataset: for negative sampling, ignore this argument in this function.\n",
    "        \n",
    "        Outputs:\n",
    "            loss: naive softmax loss\n",
    "            gradCenterVec: the gradient with respect to the center word vector (dJ/dv_c in question 2.1.1).\n",
    "            gradOutsideVecs: the gradient with respect to all the outside word vectors (dJ / dU).\n",
    "    '''\n",
    "    \n",
    "    ### Start your code (Please use the provided softmax function)\n",
    "    \n",
    "    y_hat = softmax(outsideVectors@centerWordVec)\n",
    "    loss = -np.log(y_hat[outsideWordIdx])\n",
    "    y_hat[outsideWordIdx] -= 1.0\n",
    "    gradCenterVec = outsideVectors.T@y_hat\n",
    "    gradOutsideVecs = np.outer(y_hat,centerWordVec)\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e5c1a",
   "metadata": {},
   "source": [
    "#### Negative sampling loss function for word2vec models (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2da554ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "def negSamplingLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset, K=10):\n",
    "    ''' Implement the negative sampling loss and gradients for a centerWordVec\n",
    "        and a outsideWordIdx word vector as a building block for word2vec\n",
    "        models. K is the number of negative samples to take.\n",
    "\n",
    "        Note: The same word may be negatively sampled multiple times. For\n",
    "        example if an outside word is sampled twice, you shall have to\n",
    "        double count the gradient with respect to this word. Thrice if\n",
    "        it was sampled three times, and so forth.\n",
    "\n",
    "        Inputs/Outpus Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    '''\n",
    "    \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "    \n",
    "    ### Start your code (Please use your implementation of sigmoid)\n",
    "    \n",
    "    gradCenterVec, gradOutsideVecs = np.zeros(centerWordVec.shape),np.zeros(outsideVectors.shape)\n",
    "    \n",
    "    y_hat = sigmoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec))\n",
    "    loss = -np.log(y_hat)\n",
    "    gradCenterVec += outsideVectors[outsideWordIdx] * (y_hat - 1.0)\n",
    "    gradOutsideVecs[outsideWordIdx] = centerWordVec * (y_hat - 1.0)\n",
    "    \n",
    "    for i in range(K):\n",
    "        y_hat = sigmoid(-np.dot(outsideVectors[indices[i+1]],centerWordVec))\n",
    "        loss -= np.log(y_hat)\n",
    "        gradCenterVec += outsideVectors[indices[i+1]] * (1.0 - y_hat)\n",
    "        gradOutsideVecs[indices[i+1]] += centerWordVec * (1.0 - y_hat)\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afecb94",
   "metadata": {},
   "source": [
    "#### Skip-gram model in word2vec (Fill the code, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7499d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    ''' Implement the skip-gram model in this function.\n",
    "    \n",
    "        Inputs:\n",
    "            currentCenterWord: a string of the current center word\n",
    "            windowSize: integer, context window size\n",
    "            outsideWords: list of no more than 2*windowSize strings, the outside words\n",
    "            word2Ind: a dictionary that maps words to their indices in\n",
    "                      the word vector list\n",
    "            centerWordVectors: center word vectors (as rows) for all words in vocab\n",
    "                                (V in pdf handout)\n",
    "            outsideVectors: outside word vectors (as rows) for all words in vocab\n",
    "                            (U in pdf handout)\n",
    "            word2vecLossAndGradient: the loss and gradient function for\n",
    "                                       a prediction vector given the outsideWordIdx\n",
    "                                       word vectors, could be one of the two\n",
    "                                       loss functions you implemented above.\n",
    "\n",
    "        Outputs:\n",
    "            loss: the loss function value for the skip-gram model\n",
    "                    (J in the pdf handout)\n",
    "            gradCenterVecs: the gradient with respect to the center word vectors\n",
    "                    (dJ / dV in the pdf handout)\n",
    "            gradOutsideVectors: the gradient with respect to the outside word vectors\n",
    "                                (dJ / dU in the pdf handout)\n",
    "    '''\n",
    "    \n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    # Start your code\n",
    "    \n",
    "    i = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[i]\n",
    "    \n",
    "    for ow in outsideWords:\n",
    "        j = word2Ind[ow]\n",
    "        ls, cnt_grad, out_grad = word2vecLossAndGradient(centerWordVec, j, outsideVectors, dataset)\n",
    "        loss += ls\n",
    "        gradCenterVecs[i] += cnt_grad\n",
    "        gradOutsideVectors += out_grad\n",
    "\n",
    "\n",
    "    # End\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n",
    "\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N, 1)) + 1e-30\n",
    "    return x\n",
    "\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2), :]\n",
    "    outsideVectors = wordVectors[int(N/2):, :]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb93a4",
   "metadata": {},
   "source": [
    "#### Run the following cell to test your implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca6b62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "Skip-Gram with naiveSoftmaxLossAndGradient\n",
      "Your Result:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "    \n",
      "Skip-Gram with negSamplingLossAndGradient\n",
      "Your Result:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "############################################\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "            [tokens[random.randint(0, 4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                  dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    )\n",
    "    )\n",
    "\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-1.26947339 -1.36873189  2.45158957]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.41045956  0.18834851  1.43272264]\n",
    " [ 0.38202831 -0.17530219 -1.33348241]\n",
    " [ 0.07009355 -0.03216399 -0.24466386]\n",
    " [ 0.09472154 -0.04346509 -0.33062865]\n",
    " [-0.13638384  0.06258276  0.47605228]]\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Skip-Gram with negSamplingLossAndGradient\")\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5, :],\n",
    "                  dummy_vectors[5:, :], dataset, negSamplingLossAndGradient)\n",
    "    )\n",
    "    )\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-4.54650789 -1.85942252  0.76397441]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    " Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.69148188  0.31730185  2.41364029]\n",
    " [-0.22716495  0.10423969  0.79292674]\n",
    " [-0.45528438  0.20891737  1.58918512]\n",
    " [-0.31602611  0.14501561  1.10309954]\n",
    " [-0.80620296  0.36994417  2.81407799]]\n",
    "    \"\"\")\n",
    "    \n",
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d998e0",
   "metadata": {},
   "source": [
    "#### 2.2.3 K-nearest neighbors. (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4720d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similartiy(v1, v2):\n",
    "    ''' return the cosine similarity of two vectors\n",
    "        \n",
    "        Inputs:\n",
    "            v1: a numpy ndarray\n",
    "            v2: a numpy ndarray\n",
    "        Outputs:\n",
    "            s: the cosine similarity of v1 and v2\n",
    "    '''\n",
    "    \n",
    "    ### Start your code\n",
    "    \n",
    "    s = np.dot(v1,v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return s\n",
    "\n",
    "def knn(vec, mat, k):\n",
    "    ''' Implement the KNN algorithm, which will be used for analysis.\n",
    "        \n",
    "        Inputs:\n",
    "            vec: numpy ndarray, the target vector\n",
    "            mat: numpy ndarray, a matrix contains all the vectors (each row is a vector)\n",
    "            k: the number of the nearest neighbors you want to find.\n",
    "            \n",
    "        Outputs:\n",
    "            indices: the k indices of the matrix's rows that are closest to the vec\n",
    "    '''\n",
    "                          \n",
    "    ### Start your code\n",
    "    \n",
    "    scores = np.array([cosine_similartiy(vec, v2) for v2 in mat])\n",
    "    indices = (-scores).argsort()[:k]\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a796f59",
   "metadata": {},
   "source": [
    "#### 2.2.4 Evalution the model with visualization and knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9883f",
   "metadata": {},
   "source": [
    "#### Run the following cell to train the word2vec model\n",
    "\n",
    "_Note: The training process may take a long time depending on the efficiency of your implementation. Plan accordingly!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1aff99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "from utils.treebank import StanfordSentiment\n",
    "\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        params_file = \"saved_params_%d.npy\" % st\n",
    "        state_file = \"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    params_file = \"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=1000):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a loss and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "    print('sgd')\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        def postprocessing(x): return x\n",
    "\n",
    "    exploss = None\n",
    "\n",
    "    last_time = time.time()\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        loss = None\n",
    "        loss, g = f(x)\n",
    "        x -= step * g\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f, duration %d\" % (iter, exploss, int(time.time() - last_time)))\n",
    "            last_time = time.time()\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d202db51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a7279fb03f48569b0523e4f84fcff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd\n",
      "iter 100: 26.838811, duration 23\n",
      "iter 200: 26.922662, duration 16\n",
      "iter 300: 26.674469, duration 16\n",
      "iter 400: 26.560700, duration 16\n",
      "iter 500: 26.406836, duration 16\n",
      "iter 600: 26.298790, duration 16\n",
      "iter 700: 26.432520, duration 16\n",
      "iter 800: 26.277451, duration 16\n",
      "iter 900: 26.214011, duration 16\n",
      "iter 1000: 26.191856, duration 16\n",
      "iter 1100: 26.125123, duration 13\n",
      "iter 1200: 26.183635, duration 11\n",
      "iter 1300: 25.964566, duration 12\n",
      "iter 1400: 26.130126, duration 12\n",
      "iter 1500: 26.119661, duration 12\n",
      "iter 1600: 25.865689, duration 12\n",
      "iter 1700: 25.677752, duration 12\n",
      "iter 1800: 25.476244, duration 13\n",
      "iter 1900: 25.589445, duration 14\n",
      "iter 2000: 25.506106, duration 13\n",
      "iter 2100: 25.472869, duration 13\n",
      "iter 2200: 25.340392, duration 14\n",
      "iter 2300: 25.246540, duration 14\n",
      "iter 2400: 25.194112, duration 14\n",
      "iter 2500: 25.249451, duration 13\n",
      "iter 2600: 25.294640, duration 14\n",
      "iter 2700: 25.310057, duration 14\n",
      "iter 2800: 25.257413, duration 14\n",
      "iter 2900: 25.205463, duration 15\n",
      "iter 3000: 25.145820, duration 15\n",
      "iter 3100: 24.944646, duration 14\n",
      "iter 3200: 25.029624, duration 15\n",
      "iter 3300: 24.822818, duration 15\n",
      "iter 3400: 24.907599, duration 14\n",
      "iter 3500: 24.796127, duration 14\n",
      "iter 3600: 24.786583, duration 14\n",
      "iter 3700: 24.859813, duration 14\n",
      "iter 3800: 24.863601, duration 14\n",
      "iter 3900: 24.771253, duration 14\n",
      "iter 4000: 24.749631, duration 14\n",
      "iter 4100: 24.516157, duration 15\n",
      "iter 4200: 24.473475, duration 14\n",
      "iter 4300: 24.436287, duration 15\n",
      "iter 4400: 24.497162, duration 14\n",
      "iter 4500: 24.400427, duration 15\n",
      "iter 4600: 24.372280, duration 14\n",
      "iter 4700: 24.191591, duration 14\n",
      "iter 4800: 24.047903, duration 14\n",
      "iter 4900: 24.033826, duration 14\n",
      "iter 5000: 23.825540, duration 14\n",
      "iter 5100: 23.647307, duration 15\n",
      "iter 5200: 23.440069, duration 15\n",
      "iter 5300: 23.268025, duration 14\n",
      "iter 5400: 23.025717, duration 14\n",
      "iter 5500: 23.019268, duration 14\n",
      "iter 5600: 22.787160, duration 14\n",
      "iter 5700: 22.603441, duration 14\n",
      "iter 5800: 22.476316, duration 15\n",
      "iter 5900: 22.349846, duration 14\n",
      "iter 6000: 22.099595, duration 15\n",
      "iter 6100: 21.905747, duration 15\n",
      "iter 6200: 21.690365, duration 14\n",
      "iter 6300: 21.491103, duration 15\n",
      "iter 6400: 21.323775, duration 15\n",
      "iter 6500: 21.254697, duration 15\n",
      "iter 6600: 21.123376, duration 15\n",
      "iter 6700: 21.009936, duration 14\n",
      "iter 6800: 20.906078, duration 15\n",
      "iter 6900: 20.614049, duration 14\n",
      "iter 7000: 20.424503, duration 14\n",
      "iter 7100: 20.200907, duration 14\n",
      "iter 7200: 20.062484, duration 15\n",
      "iter 7300: 19.873811, duration 15\n",
      "iter 7400: 19.895822, duration 15\n",
      "iter 7500: 19.799045, duration 14\n",
      "iter 7600: 19.732562, duration 15\n",
      "iter 7700: 19.647043, duration 15\n",
      "iter 7800: 19.590541, duration 14\n",
      "iter 7900: 19.445850, duration 14\n",
      "iter 8000: 19.284173, duration 14\n",
      "iter 8100: 19.092521, duration 14\n",
      "iter 8200: 18.950910, duration 15\n",
      "iter 8300: 18.874105, duration 14\n",
      "iter 8400: 18.616332, duration 15\n",
      "iter 8500: 18.572530, duration 14\n",
      "iter 8600: 18.523975, duration 14\n",
      "iter 8700: 18.391871, duration 14\n",
      "iter 8800: 18.199334, duration 15\n",
      "iter 8900: 18.117735, duration 15\n",
      "iter 9000: 18.068280, duration 15\n",
      "iter 9100: 17.905495, duration 14\n",
      "iter 9200: 17.740466, duration 14\n",
      "iter 9300: 17.606684, duration 14\n",
      "iter 9400: 17.576093, duration 14\n",
      "iter 9500: 17.496325, duration 15\n",
      "iter 9600: 17.452699, duration 15\n",
      "iter 9700: 17.357857, duration 15\n",
      "iter 9800: 17.165565, duration 14\n",
      "iter 9900: 16.954173, duration 14\n",
      "iter 10000: 16.750634, duration 14\n",
      "iter 10100: 16.627484, duration 14\n",
      "iter 10200: 16.567046, duration 14\n",
      "iter 10300: 16.458306, duration 14\n",
      "iter 10400: 16.435241, duration 14\n",
      "iter 10500: 16.310423, duration 14\n",
      "iter 10600: 16.260905, duration 14\n",
      "iter 10700: 16.105078, duration 14\n",
      "iter 10800: 16.039852, duration 14\n",
      "iter 10900: 16.014176, duration 14\n",
      "iter 11000: 15.844053, duration 14\n",
      "iter 11100: 15.700902, duration 14\n",
      "iter 11200: 15.577649, duration 14\n",
      "iter 11300: 15.439438, duration 15\n",
      "iter 11400: 15.425859, duration 14\n",
      "iter 11500: 15.347598, duration 14\n",
      "iter 11600: 15.384894, duration 15\n",
      "iter 11700: 15.237832, duration 15\n",
      "iter 11800: 15.160178, duration 16\n",
      "iter 11900: 15.052708, duration 15\n",
      "iter 12000: 14.898594, duration 15\n",
      "iter 12100: 14.821756, duration 15\n",
      "iter 12200: 14.749462, duration 16\n",
      "iter 12300: 14.732394, duration 15\n",
      "iter 12400: 14.672261, duration 14\n",
      "iter 12500: 14.629260, duration 14\n",
      "iter 12600: 14.501357, duration 16\n",
      "iter 12700: 14.480133, duration 15\n",
      "iter 12800: 14.453267, duration 15\n",
      "iter 12900: 14.352574, duration 15\n",
      "iter 13000: 14.275389, duration 16\n",
      "iter 13100: 14.242569, duration 14\n",
      "iter 13200: 14.270583, duration 14\n",
      "iter 13300: 14.175771, duration 14\n",
      "iter 13400: 14.152571, duration 14\n",
      "iter 13500: 14.065922, duration 15\n",
      "iter 13600: 14.055992, duration 15\n",
      "iter 13700: 14.023796, duration 15\n",
      "iter 13800: 13.900465, duration 16\n",
      "iter 13900: 13.809087, duration 15\n",
      "iter 14000: 13.674989, duration 14\n",
      "iter 14100: 13.573044, duration 15\n",
      "iter 14200: 13.522670, duration 16\n",
      "iter 14300: 13.535641, duration 15\n",
      "iter 14400: 13.526795, duration 14\n",
      "iter 14500: 13.454591, duration 15\n",
      "iter 14600: 13.460725, duration 14\n",
      "iter 14700: 13.422972, duration 15\n",
      "iter 14800: 13.385187, duration 15\n",
      "iter 14900: 13.333212, duration 15\n",
      "iter 15000: 13.215628, duration 15\n",
      "iter 15100: 13.304246, duration 15\n",
      "iter 15200: 13.241457, duration 15\n",
      "iter 15300: 13.178353, duration 15\n",
      "iter 15400: 13.118742, duration 14\n",
      "iter 15500: 13.007031, duration 15\n",
      "iter 15600: 12.970927, duration 14\n",
      "iter 15700: 12.986840, duration 15\n",
      "iter 15800: 12.929467, duration 14\n",
      "iter 15900: 12.862384, duration 14\n",
      "iter 16000: 12.837714, duration 14\n",
      "iter 16100: 12.830167, duration 14\n",
      "iter 16200: 12.746427, duration 14\n",
      "iter 16300: 12.719491, duration 14\n",
      "iter 16400: 12.686816, duration 14\n",
      "iter 16500: 12.646565, duration 14\n",
      "iter 16600: 12.609283, duration 14\n",
      "iter 16700: 12.579273, duration 14\n",
      "iter 16800: 12.589228, duration 14\n",
      "iter 16900: 12.549636, duration 14\n",
      "iter 17000: 12.506522, duration 14\n",
      "iter 17100: 12.483469, duration 14\n",
      "iter 17200: 12.426304, duration 14\n",
      "iter 17300: 12.355016, duration 14\n",
      "iter 17400: 12.309366, duration 14\n",
      "iter 17500: 12.240302, duration 14\n",
      "iter 17600: 12.158989, duration 14\n",
      "iter 17700: 12.120559, duration 14\n",
      "iter 17800: 12.143270, duration 14\n",
      "iter 17900: 12.151645, duration 14\n",
      "iter 18000: 12.262770, duration 14\n",
      "iter 18100: 12.224692, duration 14\n",
      "iter 18200: 12.164586, duration 14\n",
      "iter 18300: 12.218758, duration 14\n",
      "iter 18400: 12.173287, duration 14\n",
      "iter 18500: 12.181752, duration 14\n",
      "iter 18600: 12.144364, duration 14\n",
      "iter 18700: 12.125466, duration 14\n",
      "iter 18800: 12.111625, duration 14\n",
      "iter 18900: 12.040323, duration 14\n",
      "iter 19000: 12.062431, duration 14\n",
      "iter 19100: 12.030640, duration 14\n",
      "iter 19200: 12.025026, duration 14\n",
      "iter 19300: 12.076327, duration 14\n",
      "iter 19400: 12.086804, duration 14\n",
      "iter 19500: 12.133664, duration 14\n",
      "iter 19600: 12.061708, duration 14\n",
      "iter 19700: 12.064958, duration 14\n",
      "iter 19800: 12.037982, duration 12\n",
      "iter 19900: 12.137643, duration 12\n",
      "iter 20000: 12.202601, duration 12\n",
      "iter 20100: 12.170739, duration 11\n",
      "iter 20200: 12.119263, duration 12\n",
      "iter 20300: 12.094200, duration 12\n",
      "iter 20400: 12.106988, duration 12\n",
      "iter 20500: 12.066671, duration 13\n",
      "iter 20600: 12.028083, duration 13\n",
      "iter 20700: 11.965335, duration 12\n",
      "iter 20800: 11.938079, duration 13\n",
      "iter 20900: 11.887927, duration 14\n",
      "iter 21000: 11.898406, duration 14\n",
      "iter 21100: 11.812416, duration 14\n",
      "iter 21200: 11.827378, duration 14\n",
      "iter 21300: 11.794008, duration 14\n",
      "iter 21400: 11.827791, duration 15\n",
      "iter 21500: 11.810076, duration 14\n",
      "iter 21600: 11.810877, duration 14\n",
      "iter 21700: 11.738958, duration 14\n",
      "iter 21800: 11.769093, duration 14\n",
      "iter 21900: 11.890766, duration 14\n",
      "iter 22000: 11.852083, duration 14\n",
      "iter 22100: 11.803800, duration 15\n",
      "iter 22200: 11.707870, duration 15\n",
      "iter 22300: 11.734641, duration 14\n",
      "iter 22400: 11.708293, duration 14\n",
      "iter 22500: 11.743728, duration 14\n",
      "iter 22600: 11.658427, duration 14\n",
      "iter 22700: 11.647753, duration 14\n",
      "iter 22800: 11.654316, duration 15\n",
      "iter 22900: 11.623340, duration 15\n",
      "iter 23000: 11.615448, duration 14\n",
      "iter 23100: 11.543126, duration 14\n",
      "iter 23200: 11.526815, duration 15\n",
      "iter 23300: 11.518779, duration 15\n",
      "iter 23400: 11.545066, duration 14\n",
      "iter 23500: 11.529531, duration 15\n",
      "iter 23600: 11.505482, duration 14\n",
      "iter 23700: 11.508046, duration 15\n",
      "iter 23800: 11.532771, duration 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23900: 11.552318, duration 14\n",
      "iter 24000: 11.557196, duration 14\n",
      "iter 24100: 11.544947, duration 14\n",
      "iter 24200: 11.571553, duration 14\n",
      "iter 24300: 11.485304, duration 14\n",
      "iter 24400: 11.498506, duration 15\n",
      "iter 24500: 11.437380, duration 15\n",
      "iter 24600: 11.409179, duration 14\n",
      "iter 24700: 11.383600, duration 14\n",
      "iter 24800: 11.489163, duration 14\n",
      "iter 24900: 11.443744, duration 14\n",
      "iter 25000: 11.425755, duration 14\n",
      "iter 25100: 11.436167, duration 14\n",
      "iter 25200: 11.505710, duration 14\n",
      "iter 25300: 11.514796, duration 14\n",
      "iter 25400: 11.518827, duration 14\n",
      "iter 25500: 11.395884, duration 14\n",
      "iter 25600: 11.406640, duration 14\n",
      "iter 25700: 11.413527, duration 14\n",
      "iter 25800: 11.378377, duration 14\n",
      "iter 25900: 11.416076, duration 14\n",
      "iter 26000: 11.406413, duration 14\n",
      "iter 26100: 11.444229, duration 14\n",
      "iter 26200: 11.382147, duration 14\n",
      "iter 26300: 11.397759, duration 14\n",
      "iter 26400: 11.378495, duration 14\n",
      "iter 26500: 11.356597, duration 14\n",
      "iter 26600: 11.403630, duration 14\n",
      "iter 26700: 11.389984, duration 14\n",
      "iter 26800: 11.284175, duration 14\n",
      "iter 26900: 11.214920, duration 14\n",
      "iter 27000: 11.218562, duration 13\n",
      "iter 27100: 11.168001, duration 15\n",
      "iter 27200: 11.174305, duration 14\n",
      "iter 27300: 11.210316, duration 14\n",
      "iter 27400: 11.202137, duration 14\n",
      "iter 27500: 11.223317, duration 14\n",
      "iter 27600: 11.224328, duration 14\n",
      "iter 27700: 11.225552, duration 14\n",
      "iter 27800: 11.249936, duration 14\n",
      "iter 27900: 11.310241, duration 14\n",
      "iter 28000: 11.322226, duration 14\n",
      "iter 28100: 11.302117, duration 14\n",
      "iter 28200: 11.228761, duration 14\n",
      "iter 28300: 11.250683, duration 14\n",
      "iter 28400: 11.311914, duration 14\n",
      "iter 28500: 11.395921, duration 14\n",
      "iter 28600: 11.460248, duration 14\n",
      "iter 28700: 11.416088, duration 14\n",
      "iter 28800: 11.407406, duration 14\n",
      "iter 28900: 11.508816, duration 15\n",
      "iter 29000: 11.454735, duration 14\n",
      "iter 29100: 11.369531, duration 14\n",
      "iter 29200: 11.444816, duration 15\n",
      "iter 29300: 11.510995, duration 14\n",
      "iter 29400: 11.546726, duration 15\n",
      "iter 29500: 11.577358, duration 14\n",
      "iter 29600: 11.516343, duration 14\n",
      "iter 29700: 11.461687, duration 15\n",
      "iter 29800: 11.476720, duration 14\n",
      "iter 29900: 11.423161, duration 14\n",
      "iter 30000: 11.379655, duration 15\n",
      "iter 30100: 11.342153, duration 14\n",
      "iter 30200: 11.314574, duration 14\n",
      "iter 30300: 11.303577, duration 14\n",
      "iter 30400: 11.317362, duration 14\n",
      "iter 30500: 11.306062, duration 14\n",
      "iter 30600: 11.319925, duration 15\n",
      "iter 30700: 11.304024, duration 15\n",
      "iter 30800: 11.302533, duration 15\n",
      "iter 30900: 11.333078, duration 14\n",
      "iter 31000: 11.245775, duration 14\n",
      "iter 31100: 11.232845, duration 14\n",
      "iter 31200: 11.265642, duration 14\n",
      "iter 31300: 11.246494, duration 14\n",
      "iter 31400: 11.177957, duration 14\n",
      "iter 31500: 11.134199, duration 14\n",
      "iter 31600: 11.204678, duration 15\n",
      "iter 31700: 11.210645, duration 14\n",
      "iter 31800: 11.165608, duration 15\n",
      "iter 31900: 11.136215, duration 14\n",
      "iter 32000: 11.069794, duration 14\n",
      "iter 32100: 11.045619, duration 14\n",
      "iter 32200: 11.044968, duration 15\n",
      "iter 32300: 11.021617, duration 15\n",
      "iter 32400: 11.066674, duration 14\n",
      "iter 32500: 11.142465, duration 14\n",
      "iter 32600: 11.180336, duration 14\n",
      "iter 32700: 11.142262, duration 14\n",
      "iter 32800: 11.110740, duration 14\n",
      "iter 32900: 11.049788, duration 14\n",
      "iter 33000: 11.003327, duration 14\n",
      "iter 33100: 10.959702, duration 14\n",
      "iter 33200: 10.974435, duration 14\n",
      "iter 33300: 10.922869, duration 14\n",
      "iter 33400: 10.871545, duration 14\n",
      "iter 33500: 10.902943, duration 14\n",
      "iter 33600: 10.855419, duration 14\n",
      "iter 33700: 10.855324, duration 14\n",
      "iter 33800: 10.959924, duration 14\n",
      "iter 33900: 10.972263, duration 14\n",
      "iter 34000: 10.988378, duration 14\n",
      "iter 34100: 10.957548, duration 14\n",
      "iter 34200: 10.968029, duration 14\n",
      "iter 34300: 11.004190, duration 14\n",
      "iter 34400: 10.990288, duration 14\n",
      "iter 34500: 10.952008, duration 14\n",
      "iter 34600: 10.870548, duration 14\n",
      "iter 34700: 10.918098, duration 14\n",
      "iter 34800: 10.921505, duration 14\n",
      "iter 34900: 10.949868, duration 14\n",
      "iter 35000: 10.995636, duration 14\n",
      "iter 35100: 10.998040, duration 14\n",
      "iter 35200: 10.970842, duration 14\n",
      "iter 35300: 11.028812, duration 14\n",
      "iter 35400: 11.012968, duration 14\n",
      "iter 35500: 11.013592, duration 14\n",
      "iter 35600: 10.907871, duration 14\n",
      "iter 35700: 10.897645, duration 14\n",
      "iter 35800: 10.875145, duration 14\n",
      "iter 35900: 10.874941, duration 14\n",
      "iter 36000: 10.860981, duration 14\n",
      "iter 36100: 10.888648, duration 14\n",
      "iter 36200: 10.838078, duration 14\n",
      "iter 36300: 10.900840, duration 14\n",
      "iter 36400: 10.939338, duration 14\n",
      "iter 36500: 11.000005, duration 14\n",
      "iter 36600: 10.979188, duration 14\n",
      "iter 36700: 11.003050, duration 14\n",
      "iter 36800: 11.029485, duration 14\n",
      "iter 36900: 11.036812, duration 14\n",
      "iter 37000: 11.044257, duration 14\n",
      "iter 37100: 11.032852, duration 14\n",
      "iter 37200: 11.102645, duration 14\n",
      "iter 37300: 11.088336, duration 14\n",
      "iter 37400: 11.113356, duration 14\n",
      "iter 37500: 11.144764, duration 14\n",
      "iter 37600: 11.183801, duration 14\n",
      "iter 37700: 11.177344, duration 14\n",
      "iter 37800: 11.157974, duration 14\n",
      "iter 37900: 11.194438, duration 14\n",
      "iter 38000: 11.174548, duration 14\n",
      "iter 38100: 11.117790, duration 14\n",
      "iter 38200: 11.084933, duration 14\n",
      "iter 38300: 11.130017, duration 14\n",
      "iter 38400: 11.141521, duration 14\n",
      "iter 38500: 11.115052, duration 14\n",
      "iter 38600: 11.135375, duration 14\n",
      "iter 38700: 11.164909, duration 14\n",
      "iter 38800: 11.196392, duration 14\n",
      "iter 38900: 11.172079, duration 14\n",
      "iter 39000: 11.131362, duration 14\n",
      "iter 39100: 11.073478, duration 14\n",
      "iter 39200: 11.104035, duration 14\n",
      "iter 39300: 11.141656, duration 14\n",
      "iter 39400: 11.126403, duration 14\n",
      "iter 39500: 11.101740, duration 14\n",
      "iter 39600: 11.094474, duration 14\n",
      "iter 39700: 11.093786, duration 14\n",
      "iter 39800: 11.142294, duration 14\n",
      "iter 39900: 11.103386, duration 14\n",
      "iter 40000: 11.080514, duration 14\n",
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 5908 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.treebank import StanfordSentiment\n",
    "\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime = time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "     dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "                                     negSamplingLossAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=100)\n",
    "\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "print(\"training took %d seconds\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c656e",
   "metadata": {},
   "source": [
    "#### Run the following cell to obtain the visulaization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1dcfca04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEACAYAAAD8wQLNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtpElEQVR4nO3deZxOdf/H8ddnhlkYZOwMUY1EaTGNJXebJWQpIpX7lruF7vSLVqXSjjblTkm0KFnLrmypu8U2dCNZkxgk0o0Zxiy+vz+uyzSYYbiucZ3h/Xw85nGdc77L+cyZ5XOdc77X95hzDhERkVALC3UAIiIiAEVCHYCInH4szqoSQ1TQOkwhzSW7zUHrTzxJCUlEgi+GKG4hNWj9jaZ40PoSz9IlOxER8QQlJBER8QQlJBER8QQlJBEJneeYDMB04niWL0McjYSYEpKIhM6TtAt1COIdGmUnIqfGa9xNCp0BiOETHmA4T7OOp4kPcWTiEUpIIlLwRnERKdxMJ67nIMYEpvMJC0IdlniLEpKIFLztJBLN59RiPwDRzGA7iSGOSjxG95BE5FSwUAcg3qeEJCIFrwIL2E8L1hHFaqLZT0sqsCjUYYm3WGGc7bts2bKuevXqoQ5DRPKQlpVGWLHD3+/u2LqDP7f/CUBshVjKVi7LygUrqdOgDulp6WxctZGal9bMtb+D+w4SFR68qfHOVEuWLNnpnCsX6jjyUijvIVWvXp2kpKRQhyEieVj7y1piysYErb+UnSnUrJF7spL8M7NfQx3DseiSnYiIeIISkoiIeIISkoiIeIISkoiIeEKhHNQgIt6wacsm0tLTjtq+dftWDiQfCNp+IotGBq2v/IqKiKJalWqnfL9nMiUkETlpaelpuY6mq1m28I+IS9mZEuoQzjhBuWRnZi3MbI2ZrTezPrmUm5kN9pcvN7PLjigPN7MfzGxaMOIREZHCJ+CEZGbhwBCgJVAbuMXMah9RrSUQ7/+6G3j7iPL7gVWBxiIiIoVXMM6QEoH1zrkNzrl0YAwc9YyTdsBI57MAOMvMKgGYWRxwPTA8CLGIiEghFYx7SFWAzTnWk4H6+ahTBdgGvA48ApQ41k7M7G58Z1dUq6YbjSJnkpEjRhIdHU3HWzsetn3zr5vp2qkrXy48uYfNvjvkXbp060J0sehghOk5Rw06iSTSapm3nj+VQppLdpshOAkpt1l8j5wgL9c6ZtYa+N05t8TMrj7WTpxzw4BhAAkJCYVvAj4RyZaVlUV4eHi+6//jjn8USBzD3x5Oh84dTtuEdNSgkygOcgupoYsoF6MpfmgxGAkpGaiaYz0O2JrPOjcBbc2sFRAFlDSzj51zXYIQl4iEwOZfN3Nb+9u4NOFSVi5fSY3zajD4ncFcnXg1nbt05usvv6bb3d04q/RZvPLiK6Snp3N2jbMZ9NYgiscU58V+LzJrxiyKFCnClddeyVMvPMWrL75K8Zji9Pi/Hiz/YTkP3PsA0dHRJDb865FKWVlZvNjvReZ/M5/09HS63tWVv//z73z/zfe81v81SpcpzZqf1lD3krr8e/i/eW/oe2zftp2O13ekdJnSTJg+IYRHTSA495AWA/FmVsPMIoDOwJQj6kwB/uEfbdcA2O2c2+ace8w5F+ecq+5v96WSkUjh9/O6n+nSrQtz5s+hRIkSfDj8QwAioyKZNGsSf7vmb7zx8huMnTKWmd/M5OJLL2bYm8P4c9effD71c+Ytmsec+XO4/+H7j+r7gX89wHMvPcfUuVMP2z565GhKlCzBjK9nMP2r6Xzy4Sds2rgJgB+X/8gzA57hq8Vf8evGX1m8YDF33HMHFSpVYPz08UpGHhHwGZJzLtPMegIzgXDgPefcSjPr4S8fCswAWgHrgX1At0D3KyLeVTmuMpc3uByA9je3572h7wHQtn1bAJYsWsLa1Wtp19w3/ikjPYN6ifUoUbIEkVGRPNTzIZpc14SmLZoe1u+e3XvYvXs3DRs3BKBD5w7Mmz0PgK+//JpVP65i+uTpAOzds5dffv6FohFFuaTeJVSuUhmAOnXrsPnXzYedXYk3BOWDsc65GfiSTs5tQ3MsO+De4/TxFfBVMOIRkdAys1zXixUvBoDDceU1V/LW+28d1Xb6vOl8+9W3TP50Mu8Pe5/x08Znlznnjur7r0J4/uXnubrp1Ydt/v6b74mIiMheDw8LJzMr82S+rTPHUJqTSjwPMoSXeZAwUnmQofRnEKWYw7+YTn9e4TzeoSPrgrVbzWUnIkG3ZfMWkhb6nlk2ecJkLm94+WHl9S6vx+KFi/nl518A2L9vPz+v+5nUlFT27tlLk+ua8MyAZ/hp+U+HtSt1VilKlizJovm+h81OHDcxu+yqJlcxcsRIMjIyAN9lw32p+44ZZ0xMDCl7NSPDUXowiwcZcsw6j/FQMJMRaOogESkA8efHM370ePr06kONc2vQ9Y6uvP/O+9nlZcqWYdDbg7j3n/eSnp4OwCNPPkJMiRj+2fmfHDhwAOcc/fr3O6rv1956LXtQw9VNrs7efmvXW9m8aTMt/tYC5xyxZWN575P3jhnnbbffRpcOXShfsfyZcx9pOnEs4ROKsogMLqMIP1GesWzjQRxlqUFP/qQm+6hLH57Is5/nmUB1nqULy3mTduzi/wAjirk8wgsAPM06ijGcNJpipPE3unE1O/PqslA+wjwhIcHpibEioZfbk2ED/WyQV5wOT6k98udTpWqV5dTkLhbzPRfQnDas4VU+pwgreYgHGU5zdtGZEnyRnZDyumR3KCFVZDvfMY0mXEdtdvMWoynDCO5hJk+zhYrcTg9m8xJ9CSOFh3jjsCBHU9ytdutAl+xERM48YWziZlZTDEcR1hDDtxQF4lhNFnEn1Nc6LiaC72nMLmLJogSfsYcG/tJ07mA2AMVZQcZhH//JJSwRkSCqenbVQn92dAZIz7F8kHD/ehgHOfFbOXmMMgEgk6LZtbLwjcTOkxKSiIicvJr8QDoN+Y7S7CGMvdxASeafTFca1CAiIievCb+ziheZywTm+gc13MOsk+lKgxpE5KTl9cTY08Hp8MTYXAc19KZlCEM6Wo5BDTpDEpGTVtj/YYu3KCGJJxTkO+2t27dyIOPAcetFFo2kcoXKBRLDiTod3p2LnCglJPGEo6bJD6Lw1HDKlSl33Hqpu1ILLIYTlbJTswfImUej7ERExBN0hiQicpqKiog6/Gw7jbCcD8TzhBSyr9UrIYmInKaOug95gAOHRrR5kS7ZSaHx7pB32b9vf9DqBWrWjFm8+dqbJ9V28CuDgxyNSOGnhCSFxvC3h7N///ETTX7rBap5q+b0fKDnSbX996v/DnI0IoWfLtmJJ+1L3Uf3rt3ZtnUbB7MO0vqG1mzftp2O13ekdJnSTJg+gT69+7Bs6TLS9qdxfbvreajvQ4x4e8RR9RbNX8QHIz8gIyODuKpxPN3/aZ7o+QTbt24n/UA6t9x5C+27tGfap9MYP3I8FStVpMa5NYiIiOCFV19g1uezGPzSYNIz0ikdW5o3h79JufLlGDtqLMuXLueFV1+gV49elChRgmU/LGPH7zvo+2xfX8y/beee2+9h7969ZGVm0X9Qf+bOnEva/jSaXdGM82udz5sjTu4sS+R0o4QknjRvzjwqVqrIRxM+AnyPrh47aizjp48ntkwsAI8++SilY0uTlZXFzW1u5qcff+KOe+5g2JBh2fV2/bGLkcNHMvSToUQXi+aDYR8w6v1R9HutH6VKlyJtfxr/aPUPGjdpzMh3RjLn+znElIihU+tO1L6wNgCJDRKZ+uVUzIxPPvyEt15/i34vHv2cnu3btzNp1iTWr11Pt5u70fqG1kwcP5GrmlzF/Q/fT1ZWFvv37ad+o/q8P+x9Zn83+9QdUJFCQAlJPKlW7Vo898RzvPDUCzRt0ZT6jeofVWfqxKmM+mAUWZlZbP9tO+tWr8tOIocsWbSEjRs20u2WbgBkZmRS95K6jBkxhnlfzAPgt62/MX3CdC5OuJjSsaUBaH1Dazas3wDAtq3buOf2e/h9+++kp6dT7ezcP7Da4voWhIWFUbNWTXbs2AHAJZddwoP/epDMjEyua30dF9a9MDgHSOQ0pHtI4knnxp/L519/Tq3atej/dH8GDRh0WPmmjZt4Z/A7jJ0yljnz59DkuiakHTh6pgeHI6FBAmMmj2HM5DFMmDGBVq1bsfCbhbw/5X3GzBlDrQtrcfZ5Z+cZy5MPP0m37t2Yu2AuA98YyIEDuc/6EBEZ8dd+/XNENriiAZ9+8SkVK1fk/rvvZ/wn40/mcIicEZSQxJN+2/Yb0cWi6dC5Az3+rwcrlq0gJiaGlL2+z1Ts3buX6OLRlCxVkh2/72De7HnZbXPWq3d5PVYsW8HmjZsBSNufxi/rf6FkqZJEF4vml3W/sGLpCtL2p7EsaRn/+/N/ZGZmMmPKjOz+9uzZQ8VKFQFOOKEkb0qmbLmy3Hb7bXT+e2dWLFsBQNGiRcnIyDj5AyRyGtIlO/Gk1StX8/yTz2NhRtEiRek/qD9LFi2hS4culK9YngnTJ3Bh3Qu5JvEaqlWvxuUNLs9ue9vttx1W7/FnHufxBx8nPd33DLK7773bd9+pyc2cfe7ZXHTZRZSvWJ4ud3Wh9bWtqVipIvG14ilRqgQADz72IN27dqdipYpcdvllbP51c76/j++/+Z6hg4dSpGgRihcvzhvvvJEdY9OGTbno4os0qEHET4+fEE84cpr8YPr5158pXub4H07fkbyDi2pdRGZmJnfceged/96Zlm0Cm6k/eWtyviZ2PdK+P/dRI65GQPv2Kk0cGzpmtsQ5lxDqOPISlDMkM2sBvIHv8bTDnXMDjig3f3krYB9wu3NuqZlVBUYCFYGDwDDn3BvBiEnkRL0/5H1WLF3BgbQDXHXtVbRo3SLgPg9kHMhXMjyK4ZmJXoNNE8dKXgJOSGYWDgwBmgHJwGIzm+Kc+ylHtZZAvP+rPvC2/zUTeNCfnEoAS8xs9hFtRU6Jex++l3OrnRvqMETOWMEY1JAIrHfObXDOpQNjgHZH1GkHjHQ+C4CzzKySc26bc24pgHNuL7AKqBKEmEREpJAJRkKqAuS8y5vM0UnluHXMrDpwKbAwCDGJiEghE4x7SJbLtiNHShyzjpnFAJ8CvZxze3LdidndwN0A1arphujp5qhp8oMoKyWLHX/uOG69yKKRQY9h35/7cv/tP46oolFBjeNU2vzrZpIWJnFjpxtDHYoUMsFISMlA1RzrccDW/NYxs6L4ktEo59xnee3EOTcMGAa+UXaBhy1eUpCjrmrWqFlgfedHKAcnZGZmUqTIqf10x+ZNm5k4fqISkpywYPymLgbizawGsAXoDNx6RJ0pQE8zG4NvMMNu59w2/+i7EcAq59xrQYhFpFAYNHAQE8dNpHJcZWJjY6l7aV0aX92YPr36kLY/jbNrnM2rQ15lx+876NW9F9O/mg74zj66de7GnPlzWP7Dcp55/BlSU1OJjY1l0NBBVKhYgZta3US9+vVIWpBEs1bNmPP5HC5NuJTv//M9u3fv5tUhr1K/UX3GjhrLzGkzycrKYs2qNXTv2Z30jHQ+HfMpERERfDThI0rHlmbjho30fbAvf/zxB9HR0bz875c5r+Z5eU4o+2K/F1m/dj3NrmhGx1s6cnfPu0N8tKWwCPgeknMuE+gJzMQ3KGGcc26lmfUwsx7+ajOADcB64F3gX/7tVwB/B641s//6v1oFGpOIly1buowZU2Yw89uZDP94OMt+WAZAr+696PtsX+bMn0Ot2rV4bcBrxJ8fT3pGOr/+8isAUz6bQusbW5ORkcETDz/BsI+G8cV/vuDmv9/MwGcHZu9jz//28Onnn9LjPt+fYGZmJtO/ms4zA57htQF/vfdb89MahowYwvR50xn43ECio6OZ9e0s6iXWY8LoCQA8cv8jPPfyc3zxny948vkneeyBx7LbH5pQ9sNxH9K/X38AHn/mcRIbJjL7u9lKRnJCgnIu75ybgS/p5Nw2NMeyA+7Npd23nNQVdpHCa9H8RVzX6jqio6MBaNayGftS97F7924aNm4IQMdbO9K9a3cA2tzYhqkTp9LzgZ5M+WwKb7//Nj+v+5k1q9bQuV1nAA5mHaR8hfLZ+2jboe1h+2zV1vc+r+6ldUn+NTl7e6MrGxFTIoaYEjGUKFmCZi2bAXBBnQv46cefSE1JZcnCJdmxAKQfSM9ezm1CWZGTpamDRE6xE50dpW37tnTv2p2WbVpiZpxz3jmsWrmKmrVqMnXu1FzbFCtW7LD1iAjfxK/h4eFkZmUetR0gLCyMyMhIACzMyMrM4uDBg5QsVTLPR2XkNqGsyMnS5Koip1hiw0RmfzGbtLQ0UlNSmTtzLsWKF6PUWaVY+L3vUw+fjvmUBlc0AKD6OdUJDwvn9Zdep21735nPufHnsmvnLpIW+qbQysjIYM2qNUGPtUTJElQ9uypTJ/oSn3OOlStWHrNNTEwMqSmpQY9FTn9KSCKn2CX1LqF5y+Y0a9SMO2+7k4svvZgSJUvw+tDXee6J52jasCkrV6yk96O9s9u07dCWz8Z+Rpsb2wC+M5t3PnqHF/u9SNNGTWl+RfPs5BRsbw5/kzEjx9C0UVOuSbyGWdNnHbP+BRdeQHiRcJo2asqwN4cVSExyetLkqiIFKK9JY1NTUikeU5z9+/bTvmV7XnrjJS665KIQRHjqpexMCflQ/DPVGTG5qoicmEf+7xHWrlnLgbQDdLy14xmTjESORQlJJASGvDck1CGIeI7uIYmIiCcoIYmIiCfokp1IASrISWMLq6iIwjtxrBQsJSSRAqRHdYvkny7ZiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJyghiYiIJwQlIZlZCzNbY2brzaxPLuVmZoP95cvN7LL8thURkTNDwAnJzMKBIUBLoDZwi5nVPqJaSyDe/3U38PYJtBURkTNAMM6QEoH1zrkNzrl0YAzQ7og67YCRzmcBcJaZVcpnWxEROQMEIyFVATbnWE/2b8tPnfy0BcDM7jazJDNL2rFjR8BBi4iItwQjIVku21w+6+SnrW+jc8OccwnOuYRy5cqdYIgiIuJ1RYLQRzJQNcd6HLA1n3Ui8tFWRETOAME4Q1oMxJtZDTOLADoDU46oMwX4h3+0XQNgt3NuWz7biojIGSDgMyTnXKaZ9QRmAuHAe865lWbWw18+FJgBtALWA/uAbsdqG2hMIiJS+Jhzud6y8bSEhASXlJQU6jBERAoVM1vinEsIdRx50UwNIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCUpIIiLiCcF4QF+BszirSgxRh9brRNdh7S9rQxlSnqIioqhWpVqowxARKXQKRUIihihuIfXQatisMGLKxoQyojyl7EwJdQgiIoWSLtmJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnFI5RdrnYl7qP7l27s23rNg5mHeT+R+6nxjk1eObxZ0hNTSU2NpZBQwdRoWIFRn0wilHvjyI9I50a59Rg8LDBRBeLZurEqQwaMIiw8DBKlizJZ198RlpaGo/1fozlPywnvEg4/V7sxxVXXsHYUWOZPWM2+/ftZ+MvG2nZpiVPPPdEqA+DiMhpo9AmpHlz5lGxUkU+mvARAHt276FLhy68P+Z9ypQtw+RPJzPw2YG89tZrtGzTkttuvw2Agc8OZPTI0fyzxz95feDrjJo4ikqVK7H7f7sB+ODdDwCYu2Au69eu55YbbuGbpd8AsHLFSmZ+M5OIyAiurHcl3bp3o0pclVP/zYuIHGHTlk2kpacdu1IkkVbL4k9JQCmkuWS3+USaFNqEVKt2LZ574jleeOoFmrZoSqmzSrFm1Ro6t+sMwMGsg5SvUB6ANavW8NJzL7Fn9x5SU1O5qslVACQ0SKD3Pb1pc2MbWrZpCcDi+Yvp1r0bAOfVPI+4qnFsWL8BgMZXNaZkqZIA1Dy/Jls2b1FCEhFPSEtPO/7nM6M4mPMznQVqNMVPtElACcnMYoGxQHVgI9DJOfdnLvVaAG8A4cBw59wA//aXgTZAOvAz0M0597/87Pvc+HP5/OvP+XLWl/R/uj9XXnMlNWvVZOrcqUfV7X1Pb0Z8MoI6F9Vh7KixzP9mPgADXx/I0sVLmTtzLs0bN2fWt7NwzuW5z4iIiOzlsPAwMjMz8xOqiIjkQ6CDGvoAc51z8cBc//phzCwcGAK0BGoDt5hZbX/xbOBC51xdYC3wWH53/Nu234guFk2Hzh3o8X89+CHpB3bt3EXSwiQAMjIyWLNqDQApe1OoULECGRkZTBw3MbuPjRs2ctnll/HwEw8TWyaWrVu2Uv+K+tl1fl73M1uSt3Bu/LkneFhEROREBXrJrh1wtX/5Q+Ar4NEj6iQC651zGwDMbIy/3U/OuVk56i0AbsrvjlevXM3zTz6PhRlFixSl/6D+hBcJ56lHnmLPnj1kZWZx57/u5PwLzufhJx6m9bWtiasaR63atUhJ8U3v8/yTz/PLz7/gnKPxVY2pc1Edzqt5Hn169aFJgyaEFwln0NuDiIyMPIlDIyIiJ8KOdYnquI3N/uecOyvH+p/OudJH1LkJaOGcu9O//negvnOu5xH1pgJjnXMfH7WfWhZPKjewly4AlStWrrt48eKTjrsgpexMoWaNmqEOQ0TOMGt/WXvce0hVqlZZTm9anpKARlPcrXbrTqTJcc+QzGwOUDGXor753Iflsu2wLGhmfYFMYFSevdzJqEPlZWaV2ZLPfYuISCFx3ITknGuaV5mZbTezSs65bWZWCfg9l2rJQNUc63HA1hx9dAVaA01cIKdrIiJymEEDBzFx3EQqx1UmNjYW0ijHG9zKHrrgKEo4G+nEfcSTRn8GYaSRyXlkEUdVerOdjmSQQFGW8hi9AXiXK/mNh4BIwtnIDfSmNvuCEW+ggxqmAF39y12BybnUWQzEm1kNM4sAOvvbHRp99yjQ1jkXlG9IRERg2dJlzJgyg5nfzmT4x8NZ9sMyX0Ein/MkrXiKZkSwjmnckt3oIKV4lI6Upx+b+JBzeJeHuJpMLmA0dfiO0mynFx25mSe5jkiWMYPuwYo50EENA4BxZnYHsAnoCGBmlfEN727lnMs0s57ATHzDvt9zzq30t38TiARmmxnAAudcjwBjEhE54y2av4jrWl1HdHQ0AM1aNuPd4e/CKs5nDo/iKImjOJF8ld2oBLMpCsSxmh3s4GZWA1CENfyPOHZTiSxqMi775COCoiwJVswBJSTn3B9Ak1y2bwVa5VifAczIpd55+dpRCmk5P2R1MPqgZx+EFxURdfxKIiIFLM87IJt5nZr8k1v4icF0IpWG2WXhpAMQxkHwL/scxFEE4yBF+Q+P86+CiLlQzNRw5PQTCQkJGskmInIMiQ0TebTXo/R8sCdZmVnMnTn3UFEMVdjODoqwh/aEsy3fnV7IEubyIpOpTjs2so4ofqQyN7IhGDEXioQkIiIn5pJ6l9C8ZXOaNWpGXNU4Lr70YjYkb8iiFC8xj+n8h2SKsBp3AlP8NGYXq+nFct5iGb6pa8rzEgQnIQX0OaRQSUhIcElJSaEOQ0TEM3L7HFJqSirFY4qzf99+2rdsz/K1y9fxcPZkBgWrID6HJCIihdMj//cIa9es5UDaATre2pHlry/fH+qYjkUJSUTkNDXkvSGHrQ94fUCIIskfPTFWREQ8QQlJREQ8QZfsREROA1ERUcf/fGYaYSfz4LyTksJxHl97NCUkEZHTQLUq1Y5f6QAHTnTk26mkS3YiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJSkgiIuIJAT1+wsxigbFAdWAj0Mk592cu9VoAbwDhwHDn3IAjyh8CXgbKOed2BhKTiEh+bNqyibT0E35kT4GJiojK3yMkTmOBPg+pDzDXOTfAzPr41x/NWcHMwoEhQDMgGVhsZlOccz/5y6v6yzYFGIuISL6lpacRUzYm1GFkO+7D9c4AgV6yawd86F/+ELghlzqJwHrn3AbnXDowxt/ukEHAI4ALMBYRESnEAk1IFZxz2wD8r+VzqVMF2JxjPdm/DTNrC2xxzi073o7M7G4zSzKzpB07dgQYtoiIeM1xE5KZzTGzH3P5ane8toe6yGWbM7NiQF/gqfx04pwb5pxLcM4llCtXLp+7FhE59W5qdRPLlh79PnvsqLH0fbBvCCIqHI57D8k51zSvMjPbbmaVnHPbzKwS8Hsu1ZKBqjnW44CtwLlADWCZmR3avtTMEp1zv53A9yAi4hlZWVmhDqHQCvSS3RSgq3+5KzA5lzqLgXgzq2FmEUBnYIpzboVzrrxzrrpzrjq+xHWZkpGIhMpbr7/FiLdHANCvTz86tu4IwDdffcN9d97HpPGTaNKgCdfWv5YXnnohu118pXhefv5lWl/TmiWLlhzW59iPx9L40sZ0aNmBpAVJp+6bKYQCTUgDgGZmtg7fSLkBAGZW2cxmADjnMoGewExgFTDOObcywP2KiARd/Ub1WTh/IQDLf1jOvpR9ZGRksHj+YmqcW4MX+r3AuGnjmPXdLP679L98Me0LAPal7uP82uczbd40EhsmZve3/bftvPLiK0yePZnRk0ezdvXakHxfhUVACck594dzrolzLt7/usu/fatzrlWOejOcczWdc+c6517Io6/q+gySiIRS3UvrsuK/K0jZm0JEZAT1EuuxbOkyFs5fSMlSJWnYuCFlypahSJEitO/UngXfLQAgPDyc69tdf1R/PyT9kN0mIiKCtu3bnupvqVDRTA0iIn5FixYlrlocYz8eS0JiAomNEvn+m+/59ZdfqVK1Sp7tIqMiCQ8Pz7XMf49c8kEJSUQkhwaNGjD030Opf0V96jeqz0fvfUSdi+pw2eWXseC7Bez6YxdZWVlMmjCJho0bHrOvSxMuZf6389n1xy4yMjKYNmnaKfouCqdAZ2oQETmtJDZKZPArg0lITKBY8WJERkaS2CiRChUr8Fi/x+h4fUecc1zb/Fquu/66Y/ZVoWIFHnzsQdo2bUuFihW46OKLNArvGMy5wjdBQkJCgktK0mgVETl5a39Z67mpg2rWqFmg+zCzJc65hALdSQB0yU5ERDxBCUlERDxBCUlERDxBCUlERDxBCUlERDxBw75F5IwUFRHlqYfiRUVEhTqEkFNCEpEz0pn+uHAv0iU7ERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxBCUkERHxhIASkpnFmtlsM1vnfy2dR70WZrbGzNabWZ8jyu7zl600s5cCiUdERAqvQM+Q+gBznXPxwFz/+mHMLBwYArQEagO3mFltf9k1QDugrnOuDvBKgPGIiEghFWhCagd86F/+ELghlzqJwHrn3AbnXDowxt8O4B5ggHPuAIBz7vcA4xERkUIq0IRUwTm3DcD/Wj6XOlWAzTnWk/3bAGoCfzOzhWb2tZldnteOzOxuM0sys6QdO3YEGLaIiHjNcR/QZ2ZzgIq5FPXN5z4sl20ux/5LAw2Ay4FxZnaOc84d1cC5YcAwgISEhKPKRUSkcDtuQnLONc2rzMy2m1kl59w2M6sE5HbJLRmommM9Dtiao+wzfwJaZGYHgbKAToFERM4wgV6ymwJ09S93BSbnUmcxEG9mNcwsAujsbwcwCbgWwMxqAhHAzgBjEhGRQijQhDQAaGZm64Bm/nXMrLKZzQBwzmUCPYGZwCpgnHNupb/9e8A5ZvYjvsEOXXO7XCciIqc/K4z//xMSElxSUlKowxARKVTMbIlzLiHUceRFMzWIiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnKCGJiIgnBJSQzCzWzGab2Tr/a+k86rUwszVmtt7M+uTYfomZLTCz/5pZkpklBhKPiIgUXoGeIfUB5jrn4oG5/vXDmFk4MARoCdQGbjGz2v7il4BnnHOXAE/510VE5AwUaEJqB3zoX/4QuCGXOonAeufcBudcOjDG3w7AASX9y6WArQHGIyIihVSRANtXcM5tA3DObTOz8rnUqQJszrGeDNT3L/cCZprZK/iSY6O8dmRmdwN3A1SrVi3AsEVExGuOm5DMbA5QMZeivvnch+Wyzflf7wF6O+c+NbNOwAigaW6dOOeGAcMAEhISXG51RCR3m7ZsIi09LdRhBF1URBTVqugN6uniuAnJOZdrggAws+1mVsl/dlQJ+D2XaslA1Rzrcfx1aa4rcL9/eTwwPF9Ri8gJSUtPI6ZsTKjDCLqUnSmhDkGCKNB7SFPwJRX8r5NzqbMYiDezGmYWAXT2twNfYrrKv3wtsC7AeEREpJAK9B7SAGCcmd0BbAI6AphZZWC4c66Vcy7TzHoCM4Fw4D3n3Ep/+7uAN8ysCJCG/x6RiIiceQJKSM65P4AmuWzfCrTKsT4DmJFLvW+BeoHEICIipwfN1CAiIp6ghCQiIp6ghCRyBhv/yXiaNmxK00ZNue+u+0jelEynNp1o2rApndp0YsvmLQD06tGLPr37cNP1N9GwbkPmfzufB/71AFclXEWvHr2y+4uvFM8zjz/DdX+7jk5tOvHHzj8AGPXBKFpd1YqmjZpyV5e72L9vf3a/Tz78JG2btqVh3YZMmzQNgPvuuo+Z02dm99vzjp7MmjHrFB0VCRUlJJEz1JpVaxj8ymDGTRvHnO/n8OzAZ+n7UF9u6nwTc+bPoX2n9jz5yJPZ9Xf/uZvx08bzdP+nuf3m27nr3ruYt2geq39azY/LfwRgX+o+Lrr4ImZ+M5OGVzTktf6vAdCyTUtmfD2DOd/P4bya5zF65Ojsfrdv386kWZP4cNyH9O/XH4Bbu97K2I/HArBn9x6SFiVxbfNrT9WhkRBRQhI5Q3339Xdcf8P1xJaJBaB0bGmWLFrCjZ1uBKBD5w4smr8ou36zls0wM2rVrkXZcmW5oM4FhIWFUbNWTZI3JQMQFhZG2w5tAWh/c3sWLfC1X7NqDTdedyNNGjRh4viJrFm9JrvfFte3yO5nx44dADRs3JCNGzayc8dOJk2YRKu2rShSJNBBweJ1+gmLnKGcc1iuE6n8xeyv8ojICMCXdCIjI7O3h4WFkZmZecz2ve/pzYhPRlDnojqMHTWW+d/MP6rfQzEd0qFzBz4b+xlTPp3Cq2+9egLfmRRWOkMSOUM1vroxUydOZdcfuwD4c9efJNRPYPIE3+fbPxv3GYkNT+yJMAcPHmT6pOkATBw/kcQGvvYpe1OoULECGRkZTBw3MV99dbqtE8Pf9k3ecv4F559QHFI4Wc53JIWFme0Afj1GlbLAzlMUzsnwcnxejg28HZ93Y4skkkjOwth12PYDlCadcgCEsZ8ofmM/VXEUwcgkms2Ek0EqVSnKHiLYTRZF2UcNSrAW4LCy3VxIBDvJpARGFsX4lTCyOEAZDlCOMDIIYz+OcIqz+bC2jlj2UJlS/JgdXwo1KMoeIvkj1+8rjTAOcKCAjlpO3v3Z+uQ3vrOdc+UKOpiTVSgT0vGYWZJzLiHUceTFy/F5OTbwdnyejq2WxfMbE+hNywLd0dOs42niT6rtID4/LL51RDGaL2nDdVzK3lzbjKa4W+0KfMoxL/9swfvx5ZfuIYmI9wznb2zhNUrwTp7JSE47SkgiEjwne3Z0pDv5Brg8KH1JoXG6DmoYFuoAjsPL8Xk5NvB2fF6ODUrwcahDOCZvx+ftn63348uX0/IekogczmpZPLeQGuo4gu4U3UOSU0OX7ETOBCmkMZrioQ4j6FI4/R6DewbTGZKIiHhCob2HZGaxZjbbzNb5X0vnUe89M/vdzH48mfYFHFsLM1tjZuvNrE+O7ZeY2QIz+6+ZJZnZiX06sYDj85fd5y9baWYveS0+f/lDZubMrKxXYjOzl81stZktN7OJZnZWEGI63nEwMxvsL19uZpflt20wnGx8ZlbVzOaZ2Sr/79n9XoovR3m4mf1gZtO8FJuZnWVmE/y/b6vMrGGw4ws651yh/AJeAvr4l/sAA/OodyVwGfDjybQvqNjwPT33Z+AcIAJYBtT2l80CWvqXWwFfnepjd5z4rgHmAJH+9fJeis9fXhXfU4p/Bcp6JTagOVDEvzww0N+74x2HHL9DnwMGNAAW5rdtEI5XIPFVAi7zL5cA1nopvhzlDwCfANO8FBvwIXCnfzkCOCuY8RXEV6E9QwLa4Tvg+F9vyK2Sc+4/cMSn00+gfQHGlgisd85tcM6lA2P87QAcUNK/XArYGsTYghHfPcAA59wBAOfc7x6LD2AQ8Ai+Y+mZ2Jxzs5xzhyZ+WwDEBRjP8Y7DoZhHOp8FwFlmVimfbQN10vE557Y555YCOOf2AquAKl6JD8DM4oDrgeFBjiug2MysJL434yMAnHPpzrn/FUCMQVWYE1IF59w2AP9r+VPcPtC+qwCbc6wn89cfWy/gZTPbDLwCPBbE2IIRX03gb2a20My+NrNgf14koPjMrC2wxTm3LMhxBRzbEf6J791tIPKzr7zq5DfOUMWXzcyqA5cCCz0W3+v43vgcDHJcgcZ2DrADeN9/OXG4mXl+UIunR9mZ2RygYi5FfU91LEcKQmy5TbN86N38PUBv59ynZtYJ37ucph6KrwhQGt8lgsuBcWZ2jvNfGwhlfGZWzN9H8/zGcqpiO2IffYFMYNSJRXfi+zpGnfy0DVQg8fkKzWKAT4Fezrk9QYztuPs+Vh0zaw387pxbYmZXBzmuPPebzzpF8N2quM85t9DM3sB3ifnJXOp7hqcTknMuz3/CZrb90Gm9//T5RC8bBdQ+CLEl47vPcUgcf12a6wocuoE7npO4HFDA8SUDn/kT0CIzO4hvcscdHojvXKAGsMx8jz6IA5aaWaJz7rcQx3aoj65Aa6DJiSTxPBxzX8epE5GPtoEKJD7MrCi+ZDTKOfdZkGMLNL6bgLZm1gqIAkqa2cfOuS4eiM0Byc65Q2eUE/AlJE8rzJfspuD7x43/dfIpbh9o34uBeDOrYWYRQGd/O/D9Ql3lX74WCPYH/wKNb5I/LsysJr5/bMGcCfmk43POrXDOlXfOVXfOVcf3B3tZfpNRQcYGvlFTwKNAW+fcviDEc6yfU86Y/+EfkdUA2O2/3JiftiGLz3zvKEYAq5xzrwU5roDjc8495pyL8/+edQa+DGIyCjS234DNZnbouR1NgJ+CGFvBOFWjJ4L9BZQB5uL7Zz0XiPVvrwzMyFFvNLANyMD3z+mOY7U/xbG1wjdy6Gegb47tjYEl+EbVLATqhejY5RVfBPAx8COwFLjWS/Ed0ddGgjvKLtBjtx7fNf//+r+GBiGmo/YF9AB6+JcNGOIvXwEknMgxDFV8/r8DByzPcbxaeSW+I/q4miCPsgvCz/YSIMl//CYBpQvi5xvML30wVkREPKEwX7ITEZHTiBKSiIh4ghKSiIh4ghKSiIh4ghKSiIh4ghKSiIh4ghKSiIh4wv8DDjQzOLO+f8cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords, :], wordVectors[nWords:, :]),\n",
    "    axis=0)\n",
    "\n",
    "visualizeWords = [\n",
    "    'state', 'season', 'company', 'world', 'against', \n",
    "    'president', 'game', 'million', 'oil', 'government'\n",
    "]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U, S, V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:, 0:2])\n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i, 0], coord[i, 1], visualizeWords[i],\n",
    "             bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:, 0]), np.max(coord[:, 0])))\n",
    "plt.ylim((np.min(coord[:, 1]), np.max(coord[:, 1])))\n",
    "\n",
    "plt.savefig('word_vectors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d575d39",
   "metadata": {},
   "source": [
    "#### Run the following cell to obtain the k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8827557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \"state\" is close to ['state', 'druyun', 'nashiri', 'astronomy', 'texting', 'supplier', 'electric', 'lawsuit', 'bath', 'blizzard']\n",
      "Word: \"season\" is close to ['season', 'berlusconi', 'pam', 'padraig', 'won', 'shiite', 'ak', 'bb', 'peoplesoft', 'straight']\n",
      "Word: \"company\" is close to ['company', 'says', 'trophy', 'ship', 'global', 'media', 'chicago', 'battles', 'contravened', 'ago']\n",
      "Word: \"world\" is close to ['world', 'lake', 'indication', 'meetings', 'baghdad', 'power', 'sbux', 'outages', 'purefoy', 'nikkei']\n",
      "Word: \"against\" is close to ['against', 'shepherd', 'cable', 'crushing', 'squeeze', 'prayers', 'perhaps', 'rocks', 'shelling', 'azeri']\n",
      "Word: \"president\" is close to ['president', 'uniteds', 'purnomo', 'marek', 'zedong', 'freshly', 'delegation', 'especially', 'orwell', 'destabilize']\n",
      "Word: \"game\" is close to ['game', 'bench', 'such', 'albanians', 'proper', 'three', 'next', 'failed', 'craft', 'eagles']\n",
      "Word: \"million\" is close to ['million', 'explodes', 'technological', 'versamail', 'decides', 'tokyo', 'delivered', 'realize', 'humanitarian', 'ai']\n",
      "Word: \"oil\" is close to ['oil', 'west', 'out', 'been', 'nearly', 'baghdad', 'station', 'will', 'sony', 'safwan']\n",
      "Word: \"government\" is close to ['government', 'mica', 'rock', 'presidential', 'qaeda', 'plummet', 'gp', 'splash', 'forest', 'respected']\n"
     ]
    }
   ],
   "source": [
    "centerVectors = wordVectors[:nWords, :]\n",
    "outputVectors = wordVectors[nWords:, :]\n",
    "for word in visualizeWords:\n",
    "    idx = tokens[word]\n",
    "    vec = outputVectors[idx]\n",
    "    indices = knn(vec, outputVectors, 10)\n",
    "    closed_words = [list(tokens.keys())[i] for i in indices]\n",
    "    print('Word: \"{}\" is close to {}'.format(word, closed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97ed05",
   "metadata": {},
   "source": [
    "Analysis/Observation:\n",
    "It seems that words are starting to cluster in the vector space. Interestingly similar topics or words that are tend to be used often or have some semantic meaning are starting to group in these clusters. In my personal opinion, this might introduce some bias if vocab size is not sufficiently large since certain words for example the name Safwan and the word Oil are scored close which would not be the case if vocab size was larger. This also highlights biases in the source media since media of certain region may only refer to words, names, places, etc. in specific limited instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
